{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-excerse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the jupyter notebook! Before the tensorflow tutorial, how about excersing on jupyter notebook?<br>\n",
    "So this is the **introduction to jupyter notebook!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you can execute shell commands following by `!`,<br>\n",
    "otherwise your commands become ipython scripts.<br>\n",
    "You can *execute* them by `Ctrl+Enter` (or click `Run` buttom at the top bar).<br>\n",
    "`Ctrl+m l` to show line numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are using your default shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $SHELL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run more complex commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls;touch foobar;ls -la foobar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can edit the inside of the command by clicking. Edit the following incomplete command to remove *foobar* file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmfoobar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the commands are running, the state become \\[\\*\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!for iii in `seq 10 0`; do sleep 1; echo $iii; done; echo finished!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, you can edit textlines on this notebook by double-clicking.<br>\n",
    "If you want to insert a *text line*, click `+` bottom and select `Code` to `Markdown`. <br>\n",
    "After you edit a text line, just `Ctrl+Enter` to replace old lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also include links and pictures as Markdown format.\n",
    "![jupyter icon](http://jupyter.org/assets/main-logo.svg)\n",
    "How about creating [your own jupyter notebook](blank.ipynb)\\?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before processing, check you are using virtualenv python (/XXXXXX/tensorflow-tutorial2018/tensorflow/bin/python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, do not forget you are in *ipython*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def dice():\n",
    "    return random.randint(1,6)\n",
    "print(dice())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play with this *dice* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "stat=np.zeros((6,),dtype=\"int64\")  # creating [0,0,0,0,0,0]\n",
    "print(stat)\n",
    "N=1000\n",
    "for i in range(N):\n",
    "    stat[dice()-1]+=1\n",
    "print(stat)\n",
    "\n",
    "# Drawing a graph\n",
    "!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.bar(np.arange(1,7),stat/N, alpha=0.5)    # draw a bar graph. alpha for transparency\n",
    "x = np.arange(1, 6, 0.1)\n",
    "y = x*0+1./6\n",
    "plt.plot(x,y, color=\"red\",linestyle=\"dotted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boring graph? Ok, draw a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N,M,BINS=1000,1000,50\n",
    "means=[]\n",
    "for i in range(N):\n",
    "    stat=[]\n",
    "    for j in range(M):\n",
    "        stat.append(dice())\n",
    "    means.append(np.mean(stat))\n",
    "# Drawing a histgram\n",
    "plt.hist(means, bins=BINS, alpha=0.5) # draw a histgram (unnormalized distribution, or counts per bin) of samples.\n",
    "\n",
    "# Central limit theorem (中心極限定理)\n",
    "x = np.arange(3, 4, 0.01)\n",
    "nu,sigma=3.5,np.sqrt(35/12./M)\n",
    "y = (1/np.sqrt(2*np.pi*(sigma**2)))*np.exp(-((x-nu)**2/(2*sigma**2))) *((np.max(means)-np.min(means))/BINS*N)\n",
    "plt.plot(x,y, color=\"red\",linestyle=\"dotted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So-called *Matlab-like*!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a trick to close and re-launch ipython kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's enter the tensorflow tutorial.\n",
    "Run [train.py](http://localhost:8888/edit/train.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code reading\n",
    "Let's see the code of Tensorflow.<br>\n",
    "(Note: In many tensorflow codes, tensorflow computation graph is defined in a python class, e.g. *class Graph()* in *train.py*. We do not use class for simplicity now.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a neural network on ipython.<br>\n",
    "The neural network consists of two bi-LSTMs and 2-layer MLP for binary classification.<br>\n",
    "The inputs are word index of pairs of question and answer sentences.<br>\n",
    "If the question and answer sentence is related, the label is 1, otherwise 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from data_load import get_batch_data, load_vocab\n",
    "from hyperparams import Hyperparams as hp\n",
    "from modules import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a command for reseting tensorflow graph.<br>\n",
    "If you change your neural network, run the following code and recreate neural network from the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define placeholders. Placeholders are containers for inputs of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_q = tf.placeholder(tf.int32, shape=(None, hp.maxlen)) # Question sentence's word index\n",
    "idx_a = tf.placeholder(tf.int32, shape=(None, hp.maxlen)) # Answer sentence's word index\n",
    "print(idx_q.shape)\n",
    "print(idx_a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "Change index to embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, num_units=100,40\n",
    "# Define\n",
    "lookup_table = tf.get_variable('lookup_table',\n",
    "                                       dtype=tf.float32,\n",
    "                                       shape=[vocab_size, num_units],\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "# Lookup\n",
    "emb_q = tf.nn.embedding_lookup(lookup_table, idx_q)\n",
    "emb_a = tf.nn.embedding_lookup(lookup_table, idx_a)\n",
    "print(emb_q.shape)\n",
    "print(emb_a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will explain how to initialize *embedding* later.\n",
    "Let's define LSTMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "This is a simple rapper function for stacked bi-LSTM layers.<br>\n",
    "This works for both CPU and GPU.<br>\n",
    "https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/stack_bidirectional_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a cudnn implementation of stacked LSTM. Unfortunately, **this doesn't work on CPU**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input shape must be [mini_batch, time_sequence, embedding]\n",
    "def cudnn_stack_bilstm(input, num_stacked, dim_hidden, timesteps, dropout_rate, name):\n",
    "    # Define LSTMs\n",
    "    bilstmA = tf.contrib.cudnn_rnn.CudnnLSTM(num_stacked, dim_hidden, dropout=dropout_rate, name=\"forward_lstm\")\n",
    "    bilstmB = tf.contrib.cudnn_rnn.CudnnLSTM(num_stacked, dim_hidden, dropout=dropout_rate, name=\"backward_lstm\")\n",
    "    # Define Computation Graph\n",
    "    rnninput = tf.transpose(input, perm=[1,0,2])\n",
    "    rnnoutputA,_ = bilstmA(rnninput)\n",
    "    rnnoutputB,_ = bilstmB(rnninput[::-1])\n",
    "    # 各word毎のhidden representationが欲しい時\n",
    "    rnnoutput = tf.concat([rnnoutputA,rnnoutputB[::-1]],axis=2)\n",
    "    rnnoutput = tf.transpose(rnnoutput,perm=[1,0,2])\n",
    "    # 最初と最後のhidden representationを取ってくる\n",
    "    # rnnoutput_y = tf.concat([rnnoutputA,rnnoutputB],axis=2)[-1] # shape=[timestep, minibatch, emb]\n",
    "    return rnnoutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, let's use cudnn compagtible lstm for CPU training and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input shape must be [mini_batch, time_sequence, embedding]\n",
    "def cudnn_cp_stack_bilstm(input, num_stacked, dim_hidden, timesteps, dropout, name):\n",
    "    # Unstack to expand input to a list of 'timesteps' tensors of shape (mini_batch, embedding)\n",
    "    input = tf.unstack(input, timesteps, 1)\n",
    "    # Define lstm cells with tensorflow\n",
    "    lstm_fw_cell = [tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell(dim_hidden, reuse=tf.AUTO_REUSE) for i in range(num_stacked)]\n",
    "    lstm_bw_cell = [tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell(dim_hidden, reuse=tf.AUTO_REUSE) for i in range(num_stacked)]\n",
    "    # Define computation graph of LSTMs\n",
    "    outputs, _, _ = tf.contrib.rnn.stack_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, input, dtype=tf.float32)\n",
    "    # Re-stack at axis of the timesteps.\n",
    "    outputs=tf.stack(outputs,axis=1)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use *cudnn_cp_stack_bilstm* now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lstm_units=40\n",
    "hid_q = cudnn_cp_stack_bilstm(emb_q, 1, lstm_units, hp.maxlen, 0.5, \"q\")\n",
    "hid_a = cudnn_cp_stack_bilstm(emb_a, 1, lstm_units, hp.maxlen, 0.5, \"a\")\n",
    "print(hid_q.shape)\n",
    "print(hid_a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last dimension become twice of LSTM hidden units because of the concatenation of forward and backward LSTMs.\n",
    "Extract last and forward values of forward and backward LSTMs for a sentence representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_q = tf.concat([hid_q[:,-1,0:lstm_units+1],hid_q[:,0,lstm_units+1:]], axis=-1)\n",
    "rep_a = tf.concat([hid_a[:,-1,0:lstm_units+1],hid_a[:,0,lstm_units+1:]], axis=-1)\n",
    "print(rep_q.shape)\n",
    "print(rep_a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP\n",
    "Concatenate question and answer representation and input to a MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = tf.concat([rep_q,rep_q], axis=-1)\n",
    "hl = tf.layers.dense(rep, hp.hidden_units, activation=tf.nn.relu)\n",
    "hl = tf.nn.dropout(hl,keep_prob=(0.5))\n",
    "logits = tf.layers.dense(hl, 2)   # binary classification\n",
    "prob = tf.nn.softmax(logits)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "Calc loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = tf.placeholder(tf.int32, shape=(None))   # binary class labels\n",
    "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\n",
    "mean_loss = tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tips:** *sparse_softmax_cross_entropy_with_logits* requires index of labels while *softmax_cross_entropy_with_logits(_v2)* requires distributions of labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=hp.lr, beta1=0.9, beta2=0.98, epsilon=1e-8)\n",
    "train_op = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train neural network with very simple datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasets=[\n",
    "        [np.array([ [1,4,5,8,3]+[0 for _ in range(11)], [1,1,2,6,3]+[0 for _ in range(11)] ]),\n",
    "         np.array([ [1,2]+[0 for _ in range(14)],       [9,10]+[0 for _ in range(14)]])],\n",
    "        [np.array([ [1,4,5,5,3]+[0 for _ in range(11)], [1,4,10,5,3]+[0 for _ in range(11)] ]),\n",
    "         np.array([ [10]+[0 for _ in range(15)],        [10]+[0 for _ in range(15)]])],\n",
    "        [np.array([ [4,4,5,6,3]+[0 for _ in range(11)], [9,4,2,6,3]+[0 for _ in range(11)] ]),\n",
    "         np.array([ [1,2]+[0 for _ in range(14)],       [5,10,4,10]+[0 for _ in range(12)]])],\n",
    "        [np.array([ [4,4,5,8,3]+[0 for _ in range(11)], [1,4,10,5,3]+[0 for _ in range(11)] ]),\n",
    "         np.array([ [1,2]+[0 for _ in range(14)],        [1,2]+[0 for _ in range(14)]])],\n",
    "]\n",
    "labels=[np.array([0,1]), np.array([1,1]), np.array([0,1]), np.array([0,0])]\n",
    "print(datasets)\n",
    "print(labels)\n",
    "print(label)\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    print(\"*** Epoch %d ***\"%epoch)\n",
    "    for mb,(data,lab) in enumerate(zip(datasets,labels)):\n",
    "        feed_dict={\n",
    "                idx_q:data[0],\n",
    "                idx_a:data[1],\n",
    "                label:lab,\n",
    "                }\n",
    "        log_loss,log_train_op = sess.run([mean_loss,train_op],feed_dict=feed_dict)\n",
    "        print(\"Epoch: %d, Mini-batch: %d, loss: %f\"%(epoch,mb,log_loss))\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tips:** *sess.run(fetches,feed_dict)* is a function to execute real computations and evaluate (get) tensors specified as *fetches*, given the tensors by *feed_dict*. This can be used for training, testing and other computation operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with your net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data,lab in zip(datasets,labels):\n",
    "    feed_dict={\n",
    "            idx_q:data[0],\n",
    "            idx_a:data[1],\n",
    "            label:lab,\n",
    "            }\n",
    "    log_prob = sess.run([prob],feed_dict=feed_dict)\n",
    "    log_class = np.argmax(log_prob,axis=-1)\n",
    "    print(\"Probability: \"+str(log_prob))\n",
    "    print(\"Argmax: \"+str(log_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, this dataset is too tiny to train this network..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign default values to the lookup table.\n",
    "Use sess.run() after creating a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lookup table.\n",
    "embedding_shape=(vocab_size, num_units)\n",
    "emb_w = tf.Variable(tf.constant(0.0, shape=embedding_shape), name=\"emb_w\")\n",
    "embedding_placeholder_w = tf.placeholder(tf.float32, embedding_shape)\n",
    "embedding_init_w = emb_w.assign(embedding_placeholder_w)      # `assign` operation.\n",
    "# ...\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# Run assign computation graph.\n",
    "# This should be once.\n",
    "init_wmat=np.random.randn(*embedding_shape)                   # Of course, you can swap this with gensim vec. \n",
    "sess.run([embedding_init_w], feed_dict={embedding_placeholder_w: init_wmat})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a special line for tensorboard. You don't need to run below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.FileWriter(logdir='./graph/', graph=tf.Graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir='./graph/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open *localhost:PORT_NUM* on your browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips\n",
    "This is the BasicLSTM of Tensorflow. You might see in many websites introducing tensorflow.<br>\n",
    "However, experts rarely use this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input shape must be [mini_batch, time_sequence, embedding]\n",
    "def simple_stack_bilstm(input, num_stacked, dim_hidden, timesteps, dropout, name):\n",
    "   # Unstack to expand input to a list of 'timesteps' tensors of shape (mini_batch, embedding)\n",
    "    input = tf.unstack(input, timesteps, 1)\n",
    "    # Define lstm cells with tensorflow\n",
    "    lstm_fw_cell = [tf.contrib.rnn.BasicLSTMCell(dim_hidden, forget_bias=1.0, reuse=tf.AUTO_REUSE, name=name+\"_f_\"+str(i)) for i in range(num_stacked)]\n",
    "    lstm_bw_cell = [tf.contrib.rnn.BasicLSTMCell(dim_hidden, forget_bias=1.0, reuse=tf.AUTO_REUSE, name=name+\"_b_\"+str(i)) for i in range(num_stacked)]\n",
    "    # Define computation graph of LSTMs\n",
    "    outputs, _, _ = tf.contrib.rnn.stack_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, input, dtype=tf.float32)\n",
    "    outputs=tf.stack(outputs,axis=1)\n",
    "    return outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
