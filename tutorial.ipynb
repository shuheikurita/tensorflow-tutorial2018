{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-excerse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the jupyter notebook! Before the tensorflow tutorial, how about excersing on jupyter notebook?<br>\n",
    "So this is the **introduction to jupyter notebook!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you can execute shell commands following by `!`,<br>\n",
    "otherwise your commands become ipython scripts.<br>\n",
    "You can *execute* them by `Ctrl+Enter` (or click `Run` buttom at the top bar).<br>\n",
    "`Ctrl+m l` to show line numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kurita/repos/tensorflow-tutorial2018\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are using your default shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/zsh\r\n"
     ]
    }
   ],
   "source": [
    "!echo $SHELL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run more complex commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md                  train.py\r\n",
      "Untitled.ipynb             train_data_a_light.txt\r\n",
      "data_load.py               train_data_label_light.txt\r\n",
      "foobar                     train_data_q_light.txt\r\n",
      "hyperparams.py             tutorial.ipynb\r\n",
      "modules.py                 vocab.txt\r\n",
      "\u001b[34mtensorflow\u001b[m\u001b[m\r\n",
      "-rw-r--r--  1 kurita  staff  0 Sep 24 23:03 foobar\r\n"
     ]
    }
   ],
   "source": [
    "!ls;touch foobar;ls -la foobar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can edit the inside of the command by clicking. Edit the following incomplete command to remove *foobar* file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmfoobar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the commands are running, the state become \\[\\*\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "finished!\n"
     ]
    }
   ],
   "source": [
    "!for iii in `seq 10 0`; do sleep 1; echo $iii; done; echo finished!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, you can edit textlines on this notebook by double-clicking.<br>\n",
    "If you want to insert a *text line*, click `+` bottom and select `Code` to `Markdown`. <br>\n",
    "After you edit a text line, just `Ctrl+Enter` to replace old lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also include links and pictures as Markdown format.\n",
    "![jupyter icon](http://jupyter.org/assets/main-logo.svg)\n",
    "How about creating [your own jupyter notebook](blank.ipynb)\\?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before processing, check you are using virtualenv python (/XXXXXX/tensorflow-tutorial2018/tensorflow/bin/python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kurita/repos/tensorflow-tutorial2018/tensorflow/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, do not forget you are in *ipython*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def dice():\n",
    "    return random.randint(1,6)\n",
    "print(dice())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play with this *dice* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0]\n",
      "[154 177 171 160 169 169]\n",
      "Requirement already satisfied: matplotlib in ./tensorflow/lib/python3.6/site-packages (3.0.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in ./tensorflow/lib/python3.6/site-packages (from matplotlib) (2.2.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./tensorflow/lib/python3.6/site-packages (from matplotlib) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in ./tensorflow/lib/python3.6/site-packages (from matplotlib) (2.7.3)\n",
      "Requirement already satisfied: numpy>=1.10.0 in ./tensorflow/lib/python3.6/site-packages (from matplotlib) (1.14.5)\n",
      "Requirement already satisfied: cycler>=0.10 in ./tensorflow/lib/python3.6/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: setuptools in ./tensorflow/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib) (39.1.0)\n",
      "Requirement already satisfied: six>=1.5 in ./tensorflow/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib) (1.11.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x121e93080>]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "stat=np.zeros((6,),dtype=\"int64\")  # creating [0,0,0,0,0,0]\n",
    "print(stat)\n",
    "N=1000\n",
    "for i in range(N):\n",
    "    stat[dice()-1]+=1\n",
    "print(stat)\n",
    "\n",
    "# Drawing a graph\n",
    "!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.bar(np.arange(1,7),stat/N, alpha=0.5)    # draw a bar graph. alpha for transparency\n",
    "x = np.arange(1, 6, 0.1)\n",
    "y = x*0+1./6\n",
    "plt.plot(x,y, color=\"red\",linestyle=\"dotted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boring graph? Ok, draw a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x121e35908>]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzt3XmYVNW57/HvSzPPIC0yNSAIylEBJRouEUXEIM7GOUZMjFzFGKO5h+g9JnHKicZENFGjRIxgNEJwACHRizgdZRCIIxABiSJzyyyj3f3eP3a1DUjT1d1Vtap2/T7P00/XqtpV+7fp4u3Vq/Zey9wdERHJfXVCBxARkdRQQRcRiQkVdBGRmFBBFxGJCRV0EZGYUEEXEYkJFXQRkZhQQRcRiQkVdBGRmKibyZ21adPGu3TpksldiojkvPnz53/u7oVVbZfRgt6lSxfmzZuXyV2KiOQ8M/s0me005CIiEhMq6CIiMaGCLiISEyroIiIxoYIuIhITKugiIjGhgi4iEhMq6CIiMaGCLiISEyrokndGT1/M6OmLQ8cQSTkVdBGRmFBBFxGJCRV0kSRomEZygQq6iEhMqKCLiMREUgXdzFqa2SQz+5eZLTKz/mbW2symm9mSxPdW6Q4rIiKVS7aHfj/worsfDvQGFgE3ATPc/TBgRqItIiKBVFnQzawFMBAYC+Duu919E3A2MC6x2TjgnHSFFBGRqiXTQ+8KFAN/NrN3zOxRM2sCtHX31Ylt1gBt0xVSRESqlkxBrwscA/zR3fsC29hneMXdHfD9PdnMRpjZPDObV1xcXNu8IiJSiWQK+gpghbvPSbQnERX4tWbWDiDxfd3+nuzuY9y9n7v3KyysctFqERGpoSoLuruvAT4zs56JuwYDC4EpwPDEfcOByWlJKCIiSamb5HbXAU+aWX1gGfB9ol8GE83sSuBT4ML0RBQRkWQkVdDd/V2g334eGpzaOCIiUlO6UlREJCZU0EVEYkIFXUQkJlTQRURiQgVdRCQmVNBFRGJCBV1EJCZU0EVEYkIFXUQkJlTQRURiQgVdRCQmkp2cSyQvjZ6+OHQEkaSphy4iEhMq6CLl1qyBkpLQKURqTAVdpNw998App8D27aGTiNSIxtBFSkqgbl0YPBiKi6Fhw9CJRGpEPXTJb888A8ceC+vWwbBhMH481KkDGzbA5s2h04lUiwq65LfCQjjkkL175WVlUXEfNSpcLpEa0JCL5LeBA+GEE8Cs4r46deCSS6Bz53C5RGpAPXTJX5Mnw5YtexfzctdfD+eck/lMIrWggi55qcXKT6OC/eijlW+0aRNF/5yZuVAitaSCLnlpc/simDkTLruM0dMX7/+K0F/8grN//r+pv+2LzAcUqQGNoUt+MoP+/RONTfvf5vrrmdhjILsbN8lYLJHaUA9d8k6bZf+i/7j7Yf36A2/YrRtrex69/zF2kSyUVEE3s0/M7AMze9fM5iXua21m081sSeJ7q/RGFUmN9gv+Sb+/jY3OZqlCk8/XMuCxe2m88fMMJBOpner00Ae5ex9375do3wTMcPfDgBmJtkjWe//MS3lkwlvQquo+SINtW+k38VHaLXgnA8lEaqc2Y+hnAyclbo8DXgN+Vss8Ihmxu0mzpLbb0Lk7j0x8i53N9QeoZL9ke+gO/D8zm29mIxL3tXX31Ynba4C2+3uimY0ws3lmNq+4uLiWcUVqadw4Tr/jxxTs3pX0U1TMJVckW9C/5e7HAKcB15rZwD0fdHcnKvpf4+5j3L2fu/crLCysXVqR2tqyhSYbiimt3yDpp7Ra/jHDfvUTWi//OI3BRGovqSEXd1+Z+L7OzJ4DjgPWmlk7d19tZu2AdWnMKZIa113HxMO/Xa2nlNWrT7tF79H087VpCiWSGlX20M2siZk1K78NnAp8CEwBhic2Gw5MTldIkZA2t+vE2L+8yvJj/lfoKCIHlMyQS1vgTTN7D3gbmObuLwJ3AUPMbAlwSqItkr0mTYLevWm6bnWVm1Z69ahIFqtyyMXdlwG993P/emBwOkKJpEXTptCpE9tbt6n2U4vmv8UJf/oNzHodDj44DeFEak9Xikr+GDoUpk6lrG69aj91d+Om7Gh5EGzcmIZgIqmhgi75obS0VgtArzmiN8/e9Rj07JnCUCKppYIu+eGtt6Bly+h7bZSVpSaPSBqooEt+aNMGfvCDWvWwj37hqWi5ut27UxhMJHU0fa7kh1694Pe/TzQ21OglNnbsyrv9T2Xm1PfY1awFNwzpkbp8Iimggi7x5w7Ll0NRUa2mwv2sb38+69u/6g1FAtGQi8Tf0qXQpQuMH5+Sl6u3XSsYSXZSQZf4a9ECHngATjyx1i916m9v5rsjz0tBKJHU05CLxFL5VZ43DOkRXQh07bWVblNZe3+WDhjCuu69omEckSyjgi7x9+GH0L49tG5d65da1v/kFAQSSQ8NuUj8nXnmfnvoNVV/2xc02ljFeqQiAaiHLvH3yCPQvHlqXsudH353IAuHnAcX6owXyS4q6BJ/p56autcy47WRt7CxY1f6pu5VRVJCBV3ibdEi2LoV+vWDOqkZYVx4qs5ykeykgi6x9t6o2+n56jQabt2cstcs2L2bgz5dwgPbv+DLxk11xahkDX0oKrH29sVXM+W2h2p1hei+Dvnofb577Xl0+HB+yl5TJBXUQ5dY++LgdnxxcLuUvua67kcw9Zb7WHvYkSl9XZHaUg9dYqvh5g30fOUFGm6u2WRclfmyUROWDDyNHa0OSunritSWCrrEVvuF7zDsrv9Dy1XLU/7azdaupMvbr6f8dUVqQwVdYuuTbwxk/CMvUNztiJS/9tHTJnDWL0dSoLnRJYtoDF1iq6xuPdZ3Tc8ZKB+cdgGLBw6lrKAgLa8vUhPqoUts9Xn+CQ5Z9G5aXntLu04Ud++Fq6BLFlFBl1gq2L2bgY/cRde5b6RtH53n/Q+d3pmVttcXqS4NuUgsldavz0PPzqWg5Mu07WPAY/eyo0VrGDU8bfsQqY6kC7qZFQDzgJXufoaZdQWeBg4C5gPfc3d9QiRZo6RRY0rS+PrTbrmfnc1bMjLR3msOdpEAqjPkcj2waI/23cBod+8ObASuTGUwkdro/j8v0feZx9O6j83ti9jVNEWzOIqkQFIF3cw6AqcDjybaBpwMTEpsMg44Jx0BRWqi26xXOOofE9O6j8YbP6fvs4/DsmVp3Y9IspIdcrkPGAU0S7QPAja5e/lftCuADvt7opmNAEYAFBUV1TypSDW8NOpuCnbt/KqdzPJy1dVg62ZOevjX8K3/gEMPTfnri1RXlT10MzsDWOfuNZqJyN3HuHs/d+9XWFhYk5cQqZHSBg3T+vqbOnTh4Qkz4dJL07ofkWQlM+QyADjLzD4h+hD0ZOB+oKWZlffwOwIr05JQpLqWLWPw/b+g1fKP07obLyiI5nNJ4UyOIrVRZUF395vdvaO7dwEuBl5x9+8CrwLnJzYbDkxOW0qR6vj0Uw574yXq7THkki5F896E229P+35EklGbC4t+BtxoZkuJxtTHpiaSSC0NGsTDk2azrnuvtO+q4wdz4b77oCSdJ0iKJKdaBd3dX3P3MxK3l7n7ce7e3d0vcPdd6YkoUgNmGRkKmfPda2H9eqira/QkPF36L/EzciRHT/1rRnZVWr++xtAla6hbIfGzcCFN23tm9uUOo0bBscdC676Z2adIJVTQJX5ee42ZaTjvfL/MYMoUqFMHBqugS1gq6CK1tWhRVNgz9UtEpBIaQ5d4GTcOTj+dgt0Z/IxeY+iSJVTQJV527YKtWymt3yBz+5w7F847j6bFazK3T5H9UEGXeBkxAt5I36IW+7V7NyxcSKPNGzK7X5F9aAxdYiUdk3BVuc/thfCHKRnfr8i+1EOX+Cgu5vIrT+PQWa+ETiIShAq6xMf27WzsdCg7m2V+0Ynj//IAQ353c8b3K7InDblIfHTuzAu3Phhk13VKSyn4Mn3rl4okQwVdcl6IcfN9zRp+fegIIhpykfg44/YfM+xXPwkdQyQYFXSJjbU9jszIlLn703DzBi657nx6vvJCkP2LgIZcJEbmXjwi2L53NW3BzmYtKGnQKFgGERV0iQUrLcXr1Al2Gb4XFPDcf2uNFwlLQy4SC93fms7Ic49N+zqiItlMBV1iYcshHVg45Dy2FrYLluGoqU8z4qIB0VQAIgFoyEViYW2Po1jb46igGTa368TH/Qdz9PbtUL9+0CySn1TQJRbqb9vK7ibNgmZYfuwAlh87gKNbtgyaQ/KXhlwk97lz1aUDOWHM3aGTREpLQyeQPKWCLjmvTmkJMy+/nn8ff1LoKFw68ly48srQMSRPachFcl5Z3Xq8850rQscAYPGJp9H2pD6hY0ieUkGXnNdo0wZK69ULPoYOMO+iEZwwpEfoGJKnqhxyMbOGZva2mb1nZgvM7LbE/V3NbI6ZLTWzCWamj/UliOOfeoirLh0I7qGjRLZvB828KAEkM4a+CzjZ3XsDfYChZvZN4G5gtLt3BzYCGjiUID466XRevfbnWbFYc8f35kCTJjBrVugokoeqLOge+SLRrJf4cuBkYFLi/nHAOWlJKFKF1b36svDU80LHAGBDUTe44w7o2DF0FMlDSZ3lYmYFZvYusA6YDnwMbHL3ksQmK4AOlTx3hJnNM7N5xcXFqcgsUmH3bgo/XkTBrp2hkwCwvVUbuOUWOPTQ0FEkDyVV0N291N37AB2B44DDk92Bu49x937u3q+wsLCGMUUqsWgRl11zDofOeTV0kgq7dsFnn4VOIXmoWuehu/sm4FWgP9DSzMrPkukIrExxNpGqFRUx9Zb7WXlkv9BJKlxxBQwaFDqF5KFkznIpNLOWiduNgCHAIqLCfn5is+HA5HSFFKlUq1YsGTiU7a2z6K+/q66CO+8MnULyUDLnobcDxplZAdEvgInuPtXMFgJPm9mdwDuAJoOWzJszh5YrN7OpQ5fQSSqcfHLoBJKnqizo7v4+0Hc/9y8jGk8XCeeqqzix8UFMvuOR0EkqlJXBsmXQuDG0bx86jeQRzeUiue2JJ5g5/PrQKfa2ezf07AmPZNEvGckLuvRfclvv3hSvy7J1PBs2hKeegj6a00UySwVdcteSJfDRRxRYZ0rrNwidZm8XXRQ6geQhDblI7nr2WTjzTAq+zMIl39avh5degpKSqrcVSREVdMld11wDc+ZkxSyLX/PCCzB0KPz736GTSB5RQZfc1bw5HJelJ1oNHQqvvaY5XSSjNIYuuckdHngABg8mK9/GhxwSfYlkkHrokptWroQf/xhefz10ksrNng2vvBI6heSRLOzaiCShQwdYuxbq14e560Kn2b+f/xw2b4a33w6dRPKECrrkJjM4+OBEI0sL+gMPRItdiGSICrrkpokTYedOuPzy0Ekq17Nn6ASSZzSGLrnpscey/9L6LVuinB99FDqJ5AkVdMlN//gHTJsWOsWB7dwJV14JL74YOonkCQ25SG4yg5YtQ6c4sMJCWLoUunQJnUTyhHroknPG/2ka8y64Mjp1MZuZQbduUFAQOonkCRV0yTmtly+lz+S/wJdfho5Stdmz4Y47QqeQPKGCLjlnycDTeGDKu9C5c+goVZs1C26/HTZsCJ1E8oDG0CUneUEBo19eEjrGfo2evvir23W7DaJs8jtc37p1wESSL9RDl9ziztC7/5NDZ74cOklSSho1pqxe/dAxJE+ooEtu2bqVth99QLPP14ZOkrQ+z42Hhx8OHUPygIZcJLc0b864x3LrvO5DZ78Kqwrh6qtDR5GYUw9dJM2e++9H4fnnQ8eQPKCCLrnll7/kpAfvDJ2iWlznoUuGVFnQzayTmb1qZgvNbIGZXZ+4v7WZTTezJYnvrdIfV/Le1q002LY1dIpqabRpAwwfrrnRJe2S6aGXAD91917AN4FrzawXcBMww90PA2Yk2iLpde+9vDTq7tApquXLho1gxgxYsSJ0FIm5Kj8UdffVwOrE7a1mtgjoAJwNnJTYbBzwGvCztKQUyWElDRupmEtGVGsM3cy6AH2BOUDbRLEHWAO0TWkykX1NngzHH0/T4jWhk4hkpaQLupk1BZ4BfuLuW/Z8zN0d8EqeN8LM5pnZvOLi4lqFlTxXrx40a8aOFrn3cc1zv/oTK488FrZtCx1FYiypgm5m9YiK+ZPu/mzi7rVm1i7xeDsqWQfM3ce4ez9371dYWJiKzJKvhg2Dl1+mtH6D0ElqxK0OrF8fOobEWDJnuRgwFljk7vfu8dAUYHji9nBgcurjiezB9/tHYE745LgT+du9T0JRUegoEmPJ9NAHAN8DTjazdxNfw4C7gCFmtgQ4JdEWSY+tW6NFocePD51EJGslc5bLm4BV8vDg1MYRqcSOHXDeedC1K+wMHaZmTnroTnh0F0yYEDqKxJTmcpHccPDBFYtC7zE9bS7Z1qoNZPmqeZLbVNAlN+zYAY0ahU5RK3MvuZpvDekROobEmOZykdwwaBBceGHoFKlRVhY6gcSUCrrkhssug7POCp2iVqy0BHr0gNtuCx1FYkpDLpIbfvSj0AlqzQvqwplnwtFHh44iMaWCLtlv3Tpo0iT6ynW/+93X7ipfg/QGja9LLWnIRbLfLbdEpyvm8IVFe9mxA3bm6LmXktVU0CX7XXYZ3HMPWGWXQ+SQBQugeXOYOjV0EokhDblI9hs4MPqKg+7dYdQoOPzw0EkkhlTQJbutXQvFxXDEERCHpdwaNIBf/Sp0CokpDblIdps4EY46ClavrnrbXFFaCgsX6nx0STn10CUrfXXmxznnQGEhdOgQOFEKjR8PP/gB/Otf0LNn6DQSIyrokt06dYKLLw6dIrVOOQXGjYvmpxFJIQ25SNaqu2M7PPNM/BaF6NQJLr8cWuXeykuS3VTQJWu1XfIhnH8+zJkTOkrqrVkDr78eOoXEjIZcJGutObwPzJ4NvXqFjpIyX3028Pc/RtMBb9lSxTNEkqceumSt0vr14fjjoVmz0FFS7+qrYcaMeFwsJVlDBV2ykzvHTnwUFi0KnSQ9evaE/v3jcW69ZA0VdMlKLVZ/xsBH74GZM0NHSZ/58+FvfwudQmJEBV2y0ub2Rfxx0uz4LGqxPw89BCNHxmfSMQlOBV2y1s7mreI5fl7u1luji4s0ji4porNcJPu4c/Lvf8lHJ53O6NBZ0qlTp8SNmJ1nL8Gohy7ZZ/Vqur85nZYrPw2dJP2efZa+z40LnUJiQgVdsk/79oyZ8BaLhpwTOkn6TZvGUdMmaBxdUqLKgm5mj5nZOjP7cI/7WpvZdDNbkviua5gltcwoq1svdIr0u/9+xv9pmsbRJSWS6aE/Dgzd576bgBnufhgwI9EWqT13GDqUHq/9PXSStBo9fXH0NWuVirmkTJUF3d3fADbsc/fZQPnA3zggD/42lozYuBG2b6egZHfoJBlzzKTH+NbY34aOITFQ07Nc2rp7+YoDa4C2Kcoj+a51a3jjDRYl5jzJBy1XfUqT9cWhY0gM1Pq0RXd3M6v0Ex0zGwGMACgqKqrt7iTuysqgTn59Vv/KdbeCGTeEDiI5r6b/c9aaWTuAxPd1lW3o7mPcvZ+79yssLKzh7iQv7NoFRUUwZkzoJJlVPoauM12klmpa0KcAwxO3hwOTUxNH8trWrTBsGHTvHjpJxn1z/B9g4MDQMSTHVTnkYmZ/BU4C2pjZCuCXwF3ARDO7EvgUiPGEG5IxbdpU9M7zaAwdYGvhIdD0SPjyS6iXB6drSlpUWdDd/ZJKHhqc4iySz0pLYcUK6Nw5dJIgFpx2AacO6RE6huS4/Pr0SbLXm29Cly7w4ouhk4S1alXoBJLDVNAlOxx2GPzmNzBgQOgk4Tz0EHTsGK03KlIDmm1RssLoBV9An7Nh9mpgdZXbx9Ipp8Bvfwv164dOIjlKPXQJb9kyiv45EystCZ0krB494MYbo4urRGpABV3C+/OfOff//pAG27aGThLe7t3w0kuweXPoJJKDVNAlvJtvZtI946MVivLY6OmL+evDk2HoUHj++dBxJAdpDF3Ca9yYlUf1C50iK6w5/Gh44QX49rdDR5EcpIIuYd14I/TvDy17h06SHczgjDNCp5AcpSEXCWfHDnj5ZViwIHSS7PPEE/Bf/xU6heQYFXQJp1EjeO89uPnm0EmyzzvvwIwZUJLnZ/5ItWjIRcLYtSuaJrdePWjQIHSa7PPrX0fno2s1I6kG9dAlY8qXXQPg8cejS/1X5+lFRAcwevpiRr/xaVTMd+2KJuwSSYIKuoRx+OFw7rlwyCGhk2Svjz+O5od/8snQSSRHaMhFwjjxxOhLKte1a/RL7+ijQyeRHKEeumRUo00b4I47YPv20FGyX5068PDDcMwxoZNIjlAPXdJu9B6LVRw6awb84Xb4zncYvVJvvwMp/3ert2Mbx/31ERacei7f/74uOJLKqYcuGbXgtAtgyRLo1St0lJxRb8d2ek95kq5z3wgdRbKcukiSEXVKvqTFquVsLOoWnd0iSdveupA/Pz6dHS1bc1LoMJLV1EOXjOg//g9c+qPzafL52tBRctKOlokpdT/4AD78cO9TQEUS1EOXjHj37MvYcnB7trVpGzpKzrLSUrjwQmjVCm79sy46kq9RQZf0WrwY3Nl20MF8cMbFodPkNC8ogAkTogUwFuksIfk6DblIrRzwT/+lS6FPH77x9JjknyMHNHptQ0Yninnf58bB2sqHsPTvnH9U0CV9unWDO+7gw6Hnh04SOy1WLWfAY/fC2LGho0gWUUGX1Fq8GM4+G1asiMZ4f/pTdrQ6KHSq2NncvognH3oOfvaz6I4FC2CrlvDLd7Uq6GY21Mw+MrOlZnZTqkJJjnGP1sKEaObEuXNh4cKwmfLAxk6HQkEBlJXBBRdoYQyp+YeiZlYAPAgMAVYAc81sirvrf3LMlI/D3jCkR8WdJSVQN3r7XHTDJXDMf8ATTzB68S7qjJ1OmdUDjd+mXfnPpu01t3Np74OjO3fsgEsuof2JF7LqSC3tl09q00M/Dljq7svcfTfwNHB2amJJRpWWRkWg3IYN0QeaCe0W/JNeLz1T8fhVV8Gxx37VXHziaTBw4Fftsrr10hpXvm7t4UdX/Aw++QQ++IC6O3dG7fffhxNOqFgZas2aaN3SLVui9rZtsGpVxWIapaXRl3tGj0FqrzYFvQPw2R7tFYn70qNtW7jllop2q1Zw223R7bIyaNYM7rorau/cGbXvvTdqb94ctR94IGp//nnUHpM4+2LVqqj9+ONR+9//jtpPPRW1Fy+O2pMmRe0PPojaL7wQtefPj9ovvhi1Z82K2q+8ErVffz1qv/VW1J4+PWq//XbUnjYtar/3XtR+7rmovWhR1J4wIWqXF9nx46P2Z4l//rFjoWnTijMeHnooam/cGLVHj45WB9q2LWr/+tfRwhLl/4FvvTXavtxdd+01w1+PN15k0IN3VvwHHzQILrroq8ffOXd4VOQlOxxxBCxdyvJjB0Tt7dujn12dxH/3N96As86C5cuj9uTJ0KEDLFsWtceNi/76Kn98zJhoKK38/fXgg9C4ccX76777ovdP+fvrN7+J3p/l768774TmzSvy/eIX0KZNRfumm6L9l7vxxr2vJr72WujZs6I9YgQcdVRF+4or9p7A7JJLonVqy51//t4ze555JgwZUtH+9rfh9NMr2iefHM1yWW7AgL3e73zjG/C971W0e/eGH/6won3EETByZEW7W7eMfXhtXsPfwmZ2PjDU3X+YaH8PON7df7TPdiOAEYlmT+CjGmZtA3xew+fmsnw87nw8ZsjP49YxJ6ezuxdWtVFtLixaCXTao90xcd9e3H0MMGbf+6vLzOa5e94NCObjcefjMUN+HreOObVqM+QyFzjMzLqaWX3gYmBKamKJiEh11biH7u4lZvYj4CWgAHjM3RekLJmIiFRLreZycfe/A39PUZaq1HrYJkfl43Hn4zFDfh63jjmFavyhqIiIZBdd+i8iEhNZVdDNrKGZvW1m75nZAjO7bT/bNDCzCYnpBuaYWZfMJ02dJI/5RjNbaGbvm9kMM+scImsqJXPce2z7HTNzM8vpsyGSPWYzuzDx815gZk9lOmeqJfkeLzKzV83sncT7fFiIrKlmZgWJY5q6n8dSX8vcPWu+AAOaJm7XA+YA39xnm5HAw4nbFwMTQufOwDEPAhonbl+T68ec7HEnHmsGvAHMBvqFzp2Bn/VhwDtAq0T74NC5M3TcY4BrErd7AZ+Ezp2iY78ReAqYup/HUl7LsqqH7pEvEs16ia99B/nPBsYlbk8CBpvl7tItyRyzu7/q7uUrGswmOuc/pyX5swa4A7gb2JmpbOmS5DFfBTzo7hsTz1mXwYhpkeRxO1B+OWkLYFWG4qWNmXUETgcerWSTlNeyrCro8NWfKO8C64Dp7j5nn02+mnLA3UuAzUBOz8+axDHv6UrgH5lJll5VHbeZHQN0cvdpQQKmQRI/6x5ADzN7y8xmm9nQzKdMvSSO+1bgMjNbQXTm3HUZjpgO9wGjgLJKHk95Lcu6gu7upe7eh6gXepyZHRk6U7ole8xmdhnQD7gnk/nS5UDHbWZ1gHuBn4bKlw5J/KzrEg27nARcAvzJzFpmNmXqJXHclwCPu3tHYBjwROI9kJPM7AxgnbvPz+R+s/YfzN03Aa8C+/ZQvppywMzqEv15tj6z6dLjAMeMmZ0C/BdwlrvvynS2dKrkuJsBRwKvmdknwDeBKbn+wWi5A/ysVwBT3P1Ld/83sJiowMfCAY77SmBiYptZQEOiOU9y1QDgrMR792ngZDP7yz7bpLyWZVVBN7PC8t6ImTUimmv9X/tsNgUYnrh9PvCKJz5VyEXJHLOZ9QUeISrmOT+mClUft7tvdvc27t7F3bsQfXZwlrvPCxI4BZJ8fz9P1DvHzNoQDcEsy2DMlEvyuJcDgxPbHEFU0IszmTOV3P1md++YeO9eTFSnLttns5TXslpdKZoG7YBxicUz6gAT3X2qmd0OzHP3KcBYoj/HlgIbiP6xclkyx3wP0BT4W+Izk+XuflawxKmRzHHHTTLH/BJwqpktBEqB/3T3XP8LNJnj/inR8NINRB+QXpHLHbXKpLuW6UpREZGYyKohFxERqTkVdBGRmFBBFxGJCRV0EZFjHaOFAAAAHUlEQVSYUEEXEYkJFXQRkZhQQRcRiQkVdBGRmPj/dVoKgRL0E0AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "N,M,BINS=1000,1000,50\n",
    "means=[]\n",
    "for i in range(N):\n",
    "    stat=[]\n",
    "    for j in range(M):\n",
    "        stat.append(dice())\n",
    "    means.append(np.mean(stat))\n",
    "\n",
    "# Drawing a histgram\n",
    "plt.hist(means, bins=BINS, alpha=0.5) # draw a histgram (distribution) of samples.\n",
    "# Central limit theorem (中心極限定理)\n",
    "x = np.arange(3, 4, 0.01)\n",
    "nu,sigma=3.5,np.sqrt(35/12./M)\n",
    "y = (1/np.sqrt(2*np.pi*(sigma**2)))*np.exp(-((x-nu)**2/(2*sigma**2))) *((np.max(means)-np.min(means))/BINS*N)\n",
    "plt.plot(x,y, color=\"red\",linestyle=\"dotted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So-called *Matlab-like*!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a trick to close and re-launch ipython kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's enter the tensorflow tutorial.\n",
    "Run [train.py](http://localhost:8888/edit/train.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kurita/repos/tensorflow-tutorial2018/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "[[ 2  3  5  6 10  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  8  5  9 10  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "(<tf.Tensor 'shuffle_batch:0' shape=(1, 16) dtype=int32>, <tf.Tensor 'shuffle_batch:1' shape=(1, 16) dtype=int32>, 2)\n",
      "\n",
      "2018-09-25 19:01:50.148869: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "Graph loaded\n",
      "*** Epoch 1 ***\n",
      "Epoch: 1, Mini-batch: 0, loss: 0.693127\n",
      "Epoch: 1, Mini-batch: 1, loss: 0.868971\n",
      "Epoch: 1, Mini-batch: 2, loss: 0.733168\n",
      "Epoch: 1, Mini-batch: 3, loss: 0.780489\n",
      "*** Epoch 2 ***\n",
      "Epoch: 2, Mini-batch: 0, loss: 0.820447\n",
      "Epoch: 2, Mini-batch: 1, loss: 0.700354\n",
      "Epoch: 2, Mini-batch: 2, loss: 0.763174\n",
      "Epoch: 2, Mini-batch: 3, loss: 0.716903\n",
      "*** Epoch 3 ***\n",
      "Epoch: 3, Mini-batch: 0, loss: 0.712106\n",
      "Epoch: 3, Mini-batch: 1, loss: 0.708164\n",
      "Epoch: 3, Mini-batch: 2, loss: 0.690224\n",
      "Epoch: 3, Mini-batch: 3, loss: 0.847016\n",
      "*** Epoch 4 ***\n",
      "Epoch: 4, Mini-batch: 0, loss: 0.690541\n",
      "Epoch: 4, Mini-batch: 1, loss: 0.695096\n",
      "Epoch: 4, Mini-batch: 2, loss: 0.753282\n",
      "Epoch: 4, Mini-batch: 3, loss: 0.769935\n",
      "*** Epoch 5 ***\n",
      "Epoch: 5, Mini-batch: 0, loss: 0.734534\n",
      "Epoch: 5, Mini-batch: 1, loss: 0.844641\n",
      "Epoch: 5, Mini-batch: 2, loss: 0.672057\n",
      "Epoch: 5, Mini-batch: 3, loss: 0.785319\n",
      "*** Epoch 6 ***\n",
      "Epoch: 6, Mini-batch: 0, loss: 0.669382\n",
      "Epoch: 6, Mini-batch: 1, loss: 0.638139\n",
      "Epoch: 6, Mini-batch: 2, loss: 0.722301\n",
      "Epoch: 6, Mini-batch: 3, loss: 0.729334\n",
      "*** Epoch 7 ***\n",
      "Epoch: 7, Mini-batch: 0, loss: 0.719301\n",
      "Epoch: 7, Mini-batch: 1, loss: 0.728797\n",
      "Epoch: 7, Mini-batch: 2, loss: 0.720159\n",
      "Epoch: 7, Mini-batch: 3, loss: 0.792448\n",
      "*** Epoch 8 ***\n",
      "Epoch: 8, Mini-batch: 0, loss: 0.712782\n",
      "Epoch: 8, Mini-batch: 1, loss: 0.666384\n",
      "Epoch: 8, Mini-batch: 2, loss: 0.695023\n",
      "Epoch: 8, Mini-batch: 3, loss: 0.811442\n",
      "*** Epoch 9 ***\n",
      "Epoch: 9, Mini-batch: 0, loss: 0.790235\n",
      "Epoch: 9, Mini-batch: 1, loss: 0.651476\n",
      "Epoch: 9, Mini-batch: 2, loss: 0.844135\n",
      "Epoch: 9, Mini-batch: 3, loss: 0.719849\n",
      "*** Epoch 10 ***\n",
      "Epoch: 10, Mini-batch: 0, loss: 0.711815\n",
      "Epoch: 10, Mini-batch: 1, loss: 0.744966\n",
      "Epoch: 10, Mini-batch: 2, loss: 0.693493\n",
      "Epoch: 10, Mini-batch: 3, loss: 0.812606\n",
      "*** Epoch 11 ***\n",
      "Epoch: 11, Mini-batch: 0, loss: 0.681585\n",
      "Epoch: 11, Mini-batch: 1, loss: 0.771173\n",
      "Epoch: 11, Mini-batch: 2, loss: 0.669202\n",
      "Epoch: 11, Mini-batch: 3, loss: 0.780469\n",
      "*** Epoch 12 ***\n",
      "Epoch: 12, Mini-batch: 0, loss: 0.711194\n",
      "Epoch: 12, Mini-batch: 1, loss: 0.651656\n",
      "Epoch: 12, Mini-batch: 2, loss: 0.652015\n",
      "Epoch: 12, Mini-batch: 3, loss: 0.696772\n",
      "*** Epoch 13 ***\n",
      "Epoch: 13, Mini-batch: 0, loss: 0.715098\n",
      "Epoch: 13, Mini-batch: 1, loss: 0.669165\n",
      "Epoch: 13, Mini-batch: 2, loss: 0.676758\n",
      "Epoch: 13, Mini-batch: 3, loss: 0.702126\n",
      "*** Epoch 14 ***\n",
      "Epoch: 14, Mini-batch: 0, loss: 0.710879\n",
      "Epoch: 14, Mini-batch: 1, loss: 0.686175\n",
      "Epoch: 14, Mini-batch: 2, loss: 0.709415\n",
      "Epoch: 14, Mini-batch: 3, loss: 0.728369\n",
      "*** Epoch 15 ***\n",
      "Epoch: 15, Mini-batch: 0, loss: 0.687060\n",
      "Epoch: 15, Mini-batch: 1, loss: 0.687487\n",
      "Epoch: 15, Mini-batch: 2, loss: 0.686007\n",
      "Epoch: 15, Mini-batch: 3, loss: 0.744826\n",
      "*** Epoch 16 ***\n",
      "Epoch: 16, Mini-batch: 0, loss: 0.702138\n",
      "Epoch: 16, Mini-batch: 1, loss: 0.710784\n",
      "Epoch: 16, Mini-batch: 2, loss: 0.699245\n",
      "Epoch: 16, Mini-batch: 3, loss: 0.699556\n",
      "*** Epoch 17 ***\n",
      "Epoch: 17, Mini-batch: 0, loss: 0.728653\n",
      "Epoch: 17, Mini-batch: 1, loss: 0.668574\n",
      "Epoch: 17, Mini-batch: 2, loss: 0.690695\n",
      "Epoch: 17, Mini-batch: 3, loss: 0.697358\n",
      "*** Epoch 18 ***\n",
      "Epoch: 18, Mini-batch: 0, loss: 0.760867\n",
      "Epoch: 18, Mini-batch: 1, loss: 0.734174\n",
      "Epoch: 18, Mini-batch: 2, loss: 0.653538\n",
      "Epoch: 18, Mini-batch: 3, loss: 0.728523\n",
      "*** Epoch 19 ***\n",
      "Epoch: 19, Mini-batch: 0, loss: 0.680410\n",
      "Epoch: 19, Mini-batch: 1, loss: 0.766001\n",
      "Epoch: 19, Mini-batch: 2, loss: 0.664260\n",
      "Epoch: 19, Mini-batch: 3, loss: 0.747868\n",
      "*** Epoch 20 ***\n",
      "Epoch: 20, Mini-batch: 0, loss: 0.703013\n",
      "Epoch: 20, Mini-batch: 1, loss: 0.738563\n",
      "Epoch: 20, Mini-batch: 2, loss: 0.685775\n",
      "Epoch: 20, Mini-batch: 3, loss: 0.765543\n",
      "*** Epoch 21 ***\n",
      "Epoch: 21, Mini-batch: 0, loss: 0.697481\n",
      "Epoch: 21, Mini-batch: 1, loss: 0.697950\n",
      "Epoch: 21, Mini-batch: 2, loss: 0.681375\n",
      "Epoch: 21, Mini-batch: 3, loss: 0.740428\n",
      "*** Epoch 22 ***\n",
      "Epoch: 22, Mini-batch: 0, loss: 0.706332\n",
      "Epoch: 22, Mini-batch: 1, loss: 0.792075\n",
      "Epoch: 22, Mini-batch: 2, loss: 0.667991\n",
      "Epoch: 22, Mini-batch: 3, loss: 0.731906\n",
      "*** Epoch 23 ***\n",
      "Epoch: 23, Mini-batch: 0, loss: 0.832442\n",
      "Epoch: 23, Mini-batch: 1, loss: 0.746464\n",
      "Epoch: 23, Mini-batch: 2, loss: 0.731602\n",
      "Epoch: 23, Mini-batch: 3, loss: 0.696870\n",
      "*** Epoch 24 ***\n",
      "Epoch: 24, Mini-batch: 0, loss: 0.686969\n",
      "Epoch: 24, Mini-batch: 1, loss: 0.752833\n",
      "Epoch: 24, Mini-batch: 2, loss: 0.678988\n",
      "Epoch: 24, Mini-batch: 3, loss: 0.765235\n",
      "*** Epoch 25 ***\n",
      "Epoch: 25, Mini-batch: 0, loss: 0.707813\n",
      "Epoch: 25, Mini-batch: 1, loss: 0.726472\n",
      "Epoch: 25, Mini-batch: 2, loss: 0.644138\n",
      "Epoch: 25, Mini-batch: 3, loss: 0.736361\n",
      "*** Epoch 26 ***\n",
      "Epoch: 26, Mini-batch: 0, loss: 0.743710\n",
      "Epoch: 26, Mini-batch: 1, loss: 0.686583\n",
      "Epoch: 26, Mini-batch: 2, loss: 0.679578\n",
      "Epoch: 26, Mini-batch: 3, loss: 0.645864\n",
      "*** Epoch 27 ***\n",
      "Epoch: 27, Mini-batch: 0, loss: 0.709563\n",
      "Epoch: 27, Mini-batch: 1, loss: 0.749663\n",
      "Epoch: 27, Mini-batch: 2, loss: 0.704497\n",
      "Epoch: 27, Mini-batch: 3, loss: 0.743071\n",
      "*** Epoch 28 ***\n",
      "Epoch: 28, Mini-batch: 0, loss: 0.844582\n",
      "Epoch: 28, Mini-batch: 1, loss: 0.816271\n",
      "Epoch: 28, Mini-batch: 2, loss: 0.674208\n",
      "Epoch: 28, Mini-batch: 3, loss: 0.825067\n",
      "*** Epoch 29 ***\n",
      "Epoch: 29, Mini-batch: 0, loss: 0.708079\n",
      "Epoch: 29, Mini-batch: 1, loss: 0.714641\n",
      "Epoch: 29, Mini-batch: 2, loss: 0.681847\n",
      "Epoch: 29, Mini-batch: 3, loss: 0.731020\n",
      "*** Epoch 30 ***\n",
      "Epoch: 30, Mini-batch: 0, loss: 0.748223\n",
      "Epoch: 30, Mini-batch: 1, loss: 0.703766\n",
      "Epoch: 30, Mini-batch: 2, loss: 0.717920\n",
      "Epoch: 30, Mini-batch: 3, loss: 0.740918\n",
      "*** Epoch 31 ***\n",
      "Epoch: 31, Mini-batch: 0, loss: 0.652838\n",
      "Epoch: 31, Mini-batch: 1, loss: 0.744724\n",
      "Epoch: 31, Mini-batch: 2, loss: 0.660462\n",
      "Epoch: 31, Mini-batch: 3, loss: 0.765731\n",
      "*** Epoch 32 ***\n",
      "Epoch: 32, Mini-batch: 0, loss: 0.673555\n",
      "Epoch: 32, Mini-batch: 1, loss: 0.672616\n",
      "Epoch: 32, Mini-batch: 2, loss: 0.674758\n",
      "Epoch: 32, Mini-batch: 3, loss: 0.683048\n",
      "*** Epoch 33 ***\n",
      "Epoch: 33, Mini-batch: 0, loss: 0.660866\n",
      "Epoch: 33, Mini-batch: 1, loss: 0.684252\n",
      "Epoch: 33, Mini-batch: 2, loss: 0.670111\n",
      "Epoch: 33, Mini-batch: 3, loss: 0.739820\n",
      "*** Epoch 34 ***\n",
      "Epoch: 34, Mini-batch: 0, loss: 0.753142\n",
      "Epoch: 34, Mini-batch: 1, loss: 0.741951\n",
      "Epoch: 34, Mini-batch: 2, loss: 0.702417\n",
      "Epoch: 34, Mini-batch: 3, loss: 0.734562\n",
      "*** Epoch 35 ***\n",
      "Epoch: 35, Mini-batch: 0, loss: 0.743293\n",
      "Epoch: 35, Mini-batch: 1, loss: 0.773897\n",
      "Epoch: 35, Mini-batch: 2, loss: 0.676318\n",
      "Epoch: 35, Mini-batch: 3, loss: 0.770709\n",
      "*** Epoch 36 ***\n",
      "Epoch: 36, Mini-batch: 0, loss: 0.698600\n",
      "Epoch: 36, Mini-batch: 1, loss: 0.791020\n",
      "Epoch: 36, Mini-batch: 2, loss: 0.673293\n",
      "Epoch: 36, Mini-batch: 3, loss: 0.643342\n",
      "*** Epoch 37 ***\n",
      "Epoch: 37, Mini-batch: 0, loss: 0.754835\n",
      "Epoch: 37, Mini-batch: 1, loss: 0.666013\n",
      "Epoch: 37, Mini-batch: 2, loss: 0.681728\n",
      "Epoch: 37, Mini-batch: 3, loss: 0.666440\n",
      "*** Epoch 38 ***\n",
      "Epoch: 38, Mini-batch: 0, loss: 0.694363\n",
      "Epoch: 38, Mini-batch: 1, loss: 0.696804\n",
      "Epoch: 38, Mini-batch: 2, loss: 0.635478\n",
      "Epoch: 38, Mini-batch: 3, loss: 0.713104\n",
      "*** Epoch 39 ***\n",
      "Epoch: 39, Mini-batch: 0, loss: 0.675578\n",
      "Epoch: 39, Mini-batch: 1, loss: 0.773979\n",
      "Epoch: 39, Mini-batch: 2, loss: 0.684434\n",
      "Epoch: 39, Mini-batch: 3, loss: 0.667161\n",
      "*** Epoch 40 ***\n",
      "Epoch: 40, Mini-batch: 0, loss: 0.731557\n",
      "Epoch: 40, Mini-batch: 1, loss: 0.725986\n",
      "Epoch: 40, Mini-batch: 2, loss: 0.662914\n",
      "Epoch: 40, Mini-batch: 3, loss: 0.719999\n",
      "*** Epoch 41 ***\n",
      "Epoch: 41, Mini-batch: 0, loss: 0.712082\n",
      "Epoch: 41, Mini-batch: 1, loss: 0.682788\n",
      "Epoch: 41, Mini-batch: 2, loss: 0.668263\n",
      "Epoch: 41, Mini-batch: 3, loss: 0.718996\n",
      "*** Epoch 42 ***\n",
      "Epoch: 42, Mini-batch: 0, loss: 0.676614\n",
      "Epoch: 42, Mini-batch: 1, loss: 0.669180\n",
      "Epoch: 42, Mini-batch: 2, loss: 0.718072\n",
      "Epoch: 42, Mini-batch: 3, loss: 0.724652\n",
      "*** Epoch 43 ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43, Mini-batch: 0, loss: 0.787510\n",
      "Epoch: 43, Mini-batch: 1, loss: 0.708498\n",
      "Epoch: 43, Mini-batch: 2, loss: 0.630115\n",
      "Epoch: 43, Mini-batch: 3, loss: 0.771403\n",
      "*** Epoch 44 ***\n",
      "Epoch: 44, Mini-batch: 0, loss: 0.685204\n",
      "Epoch: 44, Mini-batch: 1, loss: 0.781949\n",
      "Epoch: 44, Mini-batch: 2, loss: 0.696118\n",
      "Epoch: 44, Mini-batch: 3, loss: 0.742108\n",
      "*** Epoch 45 ***\n",
      "Epoch: 45, Mini-batch: 0, loss: 0.738525\n",
      "Epoch: 45, Mini-batch: 1, loss: 0.734600\n",
      "Epoch: 45, Mini-batch: 2, loss: 0.650201\n",
      "Epoch: 45, Mini-batch: 3, loss: 0.671803\n",
      "*** Epoch 46 ***\n",
      "Epoch: 46, Mini-batch: 0, loss: 0.685306\n",
      "Epoch: 46, Mini-batch: 1, loss: 0.727917\n",
      "Epoch: 46, Mini-batch: 2, loss: 0.673488\n",
      "Epoch: 46, Mini-batch: 3, loss: 0.603677\n",
      "*** Epoch 47 ***\n",
      "Epoch: 47, Mini-batch: 0, loss: 0.721923\n",
      "Epoch: 47, Mini-batch: 1, loss: 0.767551\n",
      "Epoch: 47, Mini-batch: 2, loss: 0.646480\n",
      "Epoch: 47, Mini-batch: 3, loss: 0.703062\n",
      "*** Epoch 48 ***\n",
      "Epoch: 48, Mini-batch: 0, loss: 0.689187\n",
      "Epoch: 48, Mini-batch: 1, loss: 0.726311\n",
      "Epoch: 48, Mini-batch: 2, loss: 0.728182\n",
      "Epoch: 48, Mini-batch: 3, loss: 0.714178\n",
      "*** Epoch 49 ***\n",
      "Epoch: 49, Mini-batch: 0, loss: 0.702529\n",
      "Epoch: 49, Mini-batch: 1, loss: 0.735398\n",
      "Epoch: 49, Mini-batch: 2, loss: 0.685240\n",
      "Epoch: 49, Mini-batch: 3, loss: 0.670858\n",
      "*** Epoch 50 ***\n",
      "Epoch: 50, Mini-batch: 0, loss: 0.755373\n",
      "Epoch: 50, Mini-batch: 1, loss: 0.670196\n",
      "Epoch: 50, Mini-batch: 2, loss: 0.645384\n",
      "Epoch: 50, Mini-batch: 3, loss: 0.667497\n",
      "*** Epoch 51 ***\n",
      "Epoch: 51, Mini-batch: 0, loss: 0.710609\n",
      "Epoch: 51, Mini-batch: 1, loss: 0.739493\n",
      "Epoch: 51, Mini-batch: 2, loss: 0.631075\n",
      "Epoch: 51, Mini-batch: 3, loss: 0.744368\n",
      "*** Epoch 52 ***\n",
      "Epoch: 52, Mini-batch: 0, loss: 0.688275\n",
      "Epoch: 52, Mini-batch: 1, loss: 0.705750\n",
      "Epoch: 52, Mini-batch: 2, loss: 0.705968\n",
      "Epoch: 52, Mini-batch: 3, loss: 0.616116\n",
      "*** Epoch 53 ***\n",
      "Epoch: 53, Mini-batch: 0, loss: 0.722465\n",
      "Epoch: 53, Mini-batch: 1, loss: 0.775892\n",
      "Epoch: 53, Mini-batch: 2, loss: 0.593305\n",
      "Epoch: 53, Mini-batch: 3, loss: 0.698099\n",
      "*** Epoch 54 ***\n",
      "Epoch: 54, Mini-batch: 0, loss: 0.681279\n",
      "Epoch: 54, Mini-batch: 1, loss: 0.684938\n",
      "Epoch: 54, Mini-batch: 2, loss: 0.722368\n",
      "Epoch: 54, Mini-batch: 3, loss: 0.683034\n",
      "*** Epoch 55 ***\n",
      "Epoch: 55, Mini-batch: 0, loss: 0.718512\n",
      "Epoch: 55, Mini-batch: 1, loss: 0.693546\n",
      "Epoch: 55, Mini-batch: 2, loss: 0.728284\n",
      "Epoch: 55, Mini-batch: 3, loss: 0.742187\n",
      "*** Epoch 56 ***\n",
      "Epoch: 56, Mini-batch: 0, loss: 0.647792\n",
      "Epoch: 56, Mini-batch: 1, loss: 0.661414\n",
      "Epoch: 56, Mini-batch: 2, loss: 0.654309\n",
      "Epoch: 56, Mini-batch: 3, loss: 0.776097\n",
      "*** Epoch 57 ***\n",
      "Epoch: 57, Mini-batch: 0, loss: 0.748568\n",
      "Epoch: 57, Mini-batch: 1, loss: 0.653094\n",
      "Epoch: 57, Mini-batch: 2, loss: 0.713909\n",
      "Epoch: 57, Mini-batch: 3, loss: 0.661122\n",
      "*** Epoch 58 ***\n",
      "Epoch: 58, Mini-batch: 0, loss: 0.652813\n",
      "Epoch: 58, Mini-batch: 1, loss: 0.723283\n",
      "Epoch: 58, Mini-batch: 2, loss: 0.658285\n",
      "Epoch: 58, Mini-batch: 3, loss: 0.642035\n",
      "*** Epoch 59 ***\n",
      "Epoch: 59, Mini-batch: 0, loss: 0.720017\n",
      "Epoch: 59, Mini-batch: 1, loss: 0.699333\n",
      "Epoch: 59, Mini-batch: 2, loss: 0.671045\n",
      "Epoch: 59, Mini-batch: 3, loss: 0.718679\n",
      "*** Epoch 60 ***\n",
      "Epoch: 60, Mini-batch: 0, loss: 0.662593\n",
      "Epoch: 60, Mini-batch: 1, loss: 0.715064\n",
      "Epoch: 60, Mini-batch: 2, loss: 0.688236\n",
      "Epoch: 60, Mini-batch: 3, loss: 0.603819\n",
      "*** Epoch 61 ***\n",
      "Epoch: 61, Mini-batch: 0, loss: 0.646079\n",
      "Epoch: 61, Mini-batch: 1, loss: 0.677918\n",
      "Epoch: 61, Mini-batch: 2, loss: 0.710693\n",
      "Epoch: 61, Mini-batch: 3, loss: 0.652802\n",
      "*** Epoch 62 ***\n",
      "Epoch: 62, Mini-batch: 0, loss: 0.719590\n",
      "Epoch: 62, Mini-batch: 1, loss: 0.748810\n",
      "Epoch: 62, Mini-batch: 2, loss: 0.713696\n",
      "Epoch: 62, Mini-batch: 3, loss: 0.677720\n",
      "*** Epoch 63 ***\n",
      "Epoch: 63, Mini-batch: 0, loss: 0.689328\n",
      "Epoch: 63, Mini-batch: 1, loss: 0.761270\n",
      "Epoch: 63, Mini-batch: 2, loss: 0.732605\n",
      "Epoch: 63, Mini-batch: 3, loss: 0.662399\n",
      "*** Epoch 64 ***\n",
      "Epoch: 64, Mini-batch: 0, loss: 0.724352\n",
      "Epoch: 64, Mini-batch: 1, loss: 0.677895\n",
      "Epoch: 64, Mini-batch: 2, loss: 0.647711\n",
      "Epoch: 64, Mini-batch: 3, loss: 0.684895\n",
      "*** Epoch 65 ***\n",
      "Epoch: 65, Mini-batch: 0, loss: 0.615044\n",
      "Epoch: 65, Mini-batch: 1, loss: 0.691059\n",
      "Epoch: 65, Mini-batch: 2, loss: 0.638163\n",
      "Epoch: 65, Mini-batch: 3, loss: 0.751533\n",
      "*** Epoch 66 ***\n",
      "Epoch: 66, Mini-batch: 0, loss: 0.783527\n",
      "Epoch: 66, Mini-batch: 1, loss: 0.699316\n",
      "Epoch: 66, Mini-batch: 2, loss: 0.685097\n",
      "Epoch: 66, Mini-batch: 3, loss: 0.679750\n",
      "*** Epoch 67 ***\n",
      "Epoch: 67, Mini-batch: 0, loss: 0.679174\n",
      "Epoch: 67, Mini-batch: 1, loss: 0.769313\n",
      "Epoch: 67, Mini-batch: 2, loss: 0.575014\n",
      "Epoch: 67, Mini-batch: 3, loss: 0.627783\n",
      "*** Epoch 68 ***\n",
      "Epoch: 68, Mini-batch: 0, loss: 0.679811\n",
      "Epoch: 68, Mini-batch: 1, loss: 0.700352\n",
      "Epoch: 68, Mini-batch: 2, loss: 0.694012\n",
      "Epoch: 68, Mini-batch: 3, loss: 0.763000\n",
      "*** Epoch 69 ***\n",
      "Epoch: 69, Mini-batch: 0, loss: 0.655449\n",
      "Epoch: 69, Mini-batch: 1, loss: 0.741794\n",
      "Epoch: 69, Mini-batch: 2, loss: 0.694699\n",
      "Epoch: 69, Mini-batch: 3, loss: 0.611324\n",
      "*** Epoch 70 ***\n",
      "Epoch: 70, Mini-batch: 0, loss: 0.695901\n",
      "Epoch: 70, Mini-batch: 1, loss: 0.715483\n",
      "Epoch: 70, Mini-batch: 2, loss: 0.674770\n",
      "Epoch: 70, Mini-batch: 3, loss: 0.687588\n",
      "*** Epoch 71 ***\n",
      "Epoch: 71, Mini-batch: 0, loss: 0.649968\n",
      "Epoch: 71, Mini-batch: 1, loss: 0.670533\n",
      "Epoch: 71, Mini-batch: 2, loss: 0.676934\n",
      "Epoch: 71, Mini-batch: 3, loss: 0.710212\n",
      "*** Epoch 72 ***\n",
      "Epoch: 72, Mini-batch: 0, loss: 0.660006\n",
      "Epoch: 72, Mini-batch: 1, loss: 0.642923\n",
      "Epoch: 72, Mini-batch: 2, loss: 0.640522\n",
      "Epoch: 72, Mini-batch: 3, loss: 0.647647\n",
      "*** Epoch 73 ***\n",
      "Epoch: 73, Mini-batch: 0, loss: 0.658979\n",
      "Epoch: 73, Mini-batch: 1, loss: 0.683915\n",
      "Epoch: 73, Mini-batch: 2, loss: 0.708832\n",
      "Epoch: 73, Mini-batch: 3, loss: 0.677024\n",
      "*** Epoch 74 ***\n",
      "Epoch: 74, Mini-batch: 0, loss: 0.709206\n",
      "Epoch: 74, Mini-batch: 1, loss: 0.678241\n",
      "Epoch: 74, Mini-batch: 2, loss: 0.699329\n",
      "Epoch: 74, Mini-batch: 3, loss: 0.700575\n",
      "*** Epoch 75 ***\n",
      "Epoch: 75, Mini-batch: 0, loss: 0.667614\n",
      "Epoch: 75, Mini-batch: 1, loss: 0.666156\n",
      "Epoch: 75, Mini-batch: 2, loss: 0.638364\n",
      "Epoch: 75, Mini-batch: 3, loss: 0.710467\n",
      "*** Epoch 76 ***\n",
      "Epoch: 76, Mini-batch: 0, loss: 0.693824\n",
      "Epoch: 76, Mini-batch: 1, loss: 0.718192\n",
      "Epoch: 76, Mini-batch: 2, loss: 0.657394\n",
      "Epoch: 76, Mini-batch: 3, loss: 0.593141\n",
      "*** Epoch 77 ***\n",
      "Epoch: 77, Mini-batch: 0, loss: 0.724862\n",
      "Epoch: 77, Mini-batch: 1, loss: 0.746029\n",
      "Epoch: 77, Mini-batch: 2, loss: 0.613249\n",
      "Epoch: 77, Mini-batch: 3, loss: 0.641362\n",
      "*** Epoch 78 ***\n",
      "Epoch: 78, Mini-batch: 0, loss: 0.641693\n",
      "Epoch: 78, Mini-batch: 1, loss: 0.725676\n",
      "Epoch: 78, Mini-batch: 2, loss: 0.636973\n",
      "Epoch: 78, Mini-batch: 3, loss: 0.685600\n",
      "*** Epoch 79 ***\n",
      "Epoch: 79, Mini-batch: 0, loss: 0.670484\n",
      "Epoch: 79, Mini-batch: 1, loss: 0.699983\n",
      "Epoch: 79, Mini-batch: 2, loss: 0.636633\n",
      "Epoch: 79, Mini-batch: 3, loss: 0.644220\n",
      "*** Epoch 80 ***\n",
      "Epoch: 80, Mini-batch: 0, loss: 0.644897\n",
      "Epoch: 80, Mini-batch: 1, loss: 0.661960\n",
      "Epoch: 80, Mini-batch: 2, loss: 0.676534\n",
      "Epoch: 80, Mini-batch: 3, loss: 0.610361\n",
      "*** Epoch 81 ***\n",
      "Epoch: 81, Mini-batch: 0, loss: 0.705873\n",
      "Epoch: 81, Mini-batch: 1, loss: 0.725457\n",
      "Epoch: 81, Mini-batch: 2, loss: 0.718751\n",
      "Epoch: 81, Mini-batch: 3, loss: 0.624442\n",
      "*** Epoch 82 ***\n",
      "Epoch: 82, Mini-batch: 0, loss: 0.643344\n",
      "Epoch: 82, Mini-batch: 1, loss: 0.723463\n",
      "Epoch: 82, Mini-batch: 2, loss: 0.635127\n",
      "Epoch: 82, Mini-batch: 3, loss: 0.641241\n",
      "*** Epoch 83 ***\n",
      "Epoch: 83, Mini-batch: 0, loss: 0.675495\n",
      "Epoch: 83, Mini-batch: 1, loss: 0.673329\n",
      "Epoch: 83, Mini-batch: 2, loss: 0.662762\n",
      "Epoch: 83, Mini-batch: 3, loss: 0.704297\n",
      "*** Epoch 84 ***\n",
      "Epoch: 84, Mini-batch: 0, loss: 0.672545\n",
      "Epoch: 84, Mini-batch: 1, loss: 0.767394\n",
      "Epoch: 84, Mini-batch: 2, loss: 0.618915\n",
      "Epoch: 84, Mini-batch: 3, loss: 0.634983\n",
      "*** Epoch 85 ***\n",
      "Epoch: 85, Mini-batch: 0, loss: 0.686037\n",
      "Epoch: 85, Mini-batch: 1, loss: 0.708058\n",
      "Epoch: 85, Mini-batch: 2, loss: 0.604377\n",
      "Epoch: 85, Mini-batch: 3, loss: 0.696586\n",
      "*** Epoch 86 ***\n",
      "Epoch: 86, Mini-batch: 0, loss: 0.639766\n",
      "Epoch: 86, Mini-batch: 1, loss: 0.670727\n",
      "Epoch: 86, Mini-batch: 2, loss: 0.630646\n",
      "Epoch: 86, Mini-batch: 3, loss: 0.608363\n",
      "*** Epoch 87 ***\n",
      "Epoch: 87, Mini-batch: 0, loss: 0.609773\n",
      "Epoch: 87, Mini-batch: 1, loss: 0.710171\n",
      "Epoch: 87, Mini-batch: 2, loss: 0.655189\n",
      "Epoch: 87, Mini-batch: 3, loss: 0.634081\n",
      "*** Epoch 88 ***\n",
      "Epoch: 88, Mini-batch: 0, loss: 0.660548\n",
      "Epoch: 88, Mini-batch: 1, loss: 0.767861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 88, Mini-batch: 2, loss: 0.665539\n",
      "Epoch: 88, Mini-batch: 3, loss: 0.644917\n",
      "*** Epoch 89 ***\n",
      "Epoch: 89, Mini-batch: 0, loss: 0.618612\n",
      "Epoch: 89, Mini-batch: 1, loss: 0.664326\n",
      "Epoch: 89, Mini-batch: 2, loss: 0.566820\n",
      "Epoch: 89, Mini-batch: 3, loss: 0.643922\n",
      "*** Epoch 90 ***\n",
      "Epoch: 90, Mini-batch: 0, loss: 0.655020\n",
      "Epoch: 90, Mini-batch: 1, loss: 0.702801\n",
      "Epoch: 90, Mini-batch: 2, loss: 0.653781\n",
      "Epoch: 90, Mini-batch: 3, loss: 0.615004\n",
      "*** Epoch 91 ***\n",
      "Epoch: 91, Mini-batch: 0, loss: 0.613318\n",
      "Epoch: 91, Mini-batch: 1, loss: 0.652575\n",
      "Epoch: 91, Mini-batch: 2, loss: 0.611535\n",
      "Epoch: 91, Mini-batch: 3, loss: 0.634732\n",
      "*** Epoch 92 ***\n",
      "Epoch: 92, Mini-batch: 0, loss: 0.656818\n",
      "Epoch: 92, Mini-batch: 1, loss: 0.655077\n",
      "Epoch: 92, Mini-batch: 2, loss: 0.624194\n",
      "Epoch: 92, Mini-batch: 3, loss: 0.712951\n",
      "*** Epoch 93 ***\n",
      "Epoch: 93, Mini-batch: 0, loss: 0.658342\n",
      "Epoch: 93, Mini-batch: 1, loss: 0.687667\n",
      "Epoch: 93, Mini-batch: 2, loss: 0.587528\n",
      "Epoch: 93, Mini-batch: 3, loss: 0.615761\n",
      "*** Epoch 94 ***\n",
      "Epoch: 94, Mini-batch: 0, loss: 0.647145\n",
      "Epoch: 94, Mini-batch: 1, loss: 0.693990\n",
      "Epoch: 94, Mini-batch: 2, loss: 0.680558\n",
      "Epoch: 94, Mini-batch: 3, loss: 0.631611\n",
      "*** Epoch 95 ***\n",
      "Epoch: 95, Mini-batch: 0, loss: 0.667572\n",
      "Epoch: 95, Mini-batch: 1, loss: 0.673717\n",
      "Epoch: 95, Mini-batch: 2, loss: 0.625882\n",
      "Epoch: 95, Mini-batch: 3, loss: 0.607021\n",
      "*** Epoch 96 ***\n",
      "Epoch: 96, Mini-batch: 0, loss: 0.679939\n",
      "Epoch: 96, Mini-batch: 1, loss: 0.769415\n",
      "Epoch: 96, Mini-batch: 2, loss: 0.637763\n",
      "Epoch: 96, Mini-batch: 3, loss: 0.726072\n",
      "*** Epoch 97 ***\n",
      "Epoch: 97, Mini-batch: 0, loss: 0.681215\n",
      "Epoch: 97, Mini-batch: 1, loss: 0.783531\n",
      "Epoch: 97, Mini-batch: 2, loss: 0.575435\n",
      "Epoch: 97, Mini-batch: 3, loss: 0.611119\n",
      "*** Epoch 98 ***\n",
      "Epoch: 98, Mini-batch: 0, loss: 0.657153\n",
      "Epoch: 98, Mini-batch: 1, loss: 0.687566\n",
      "Epoch: 98, Mini-batch: 2, loss: 0.649183\n",
      "Epoch: 98, Mini-batch: 3, loss: 0.680304\n",
      "*** Epoch 99 ***\n",
      "Epoch: 99, Mini-batch: 0, loss: 0.695497\n",
      "Epoch: 99, Mini-batch: 1, loss: 0.709831\n",
      "Epoch: 99, Mini-batch: 2, loss: 0.665408\n",
      "Epoch: 99, Mini-batch: 3, loss: 0.619687\n",
      "Train Done.\n",
      "Probability: [array([[0.485464  , 0.514536  ],\n",
      "       [0.52034813, 0.47965193]], dtype=float32)]\n",
      "Argmax: [[1 0]]\n",
      "Probability: [array([[0.53707975, 0.46292022],\n",
      "       [0.46451756, 0.5354824 ]], dtype=float32)]\n",
      "Argmax: [[0 1]]\n",
      "Probability: [array([[0.58384556, 0.41615438],\n",
      "       [0.52709234, 0.47290772]], dtype=float32)]\n",
      "Argmax: [[0 0]]\n",
      "Probability: [array([[0.577869  , 0.42213094],\n",
      "       [0.48496   , 0.51504   ]], dtype=float32)]\n",
      "Argmax: [[0 1]]\n",
      "Test Done.\n"
     ]
    }
   ],
   "source": [
    "!python train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code reading\n",
    "Let's see the simplefied code of Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from data_load import get_batch_data, load_vocab\n",
    "from hyperparams import Hyperparams as hp\n",
    "from modules import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a command for reset tensorflow graph.<br>\n",
    "If you change your neural network, run the following code and recreate neural network from the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16)\n",
      "(?, 16)\n"
     ]
    }
   ],
   "source": [
    "idx_q = tf.placeholder(tf.int32, shape=(None, hp.maxlen))\n",
    "idx_a = tf.placeholder(tf.int32, shape=(None, hp.maxlen))\n",
    "print(idx_q.shape)\n",
    "print(idx_a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "Change to embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 40)\n",
      "(?, 16, 40)\n"
     ]
    }
   ],
   "source": [
    "vocab_size, num_units=100,40\n",
    "# Define\n",
    "lookup_table = tf.get_variable('lookup_table',\n",
    "                                       dtype=tf.float32,\n",
    "                                       shape=[vocab_size, num_units],\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "# Lookup\n",
    "emb_q = tf.nn.embedding_lookup(lookup_table, idx_q)\n",
    "emb_a = tf.nn.embedding_lookup(lookup_table, idx_a)\n",
    "print(emb_q.shape)\n",
    "print(emb_a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will explain how to initialize *embedding* later.\n",
    "Let's define LSTMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "This is a simple rapper function for stacked bi-LSTM layers.<br>\n",
    "This works for both CPU and GPU.<br>\n",
    "https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/stack_bidirectional_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, I don't recommend *simple_stack_bilstm* on GPU.<br>\n",
    "This is cudnn implementation of stacked LSTM. Unfortunately, **this doesn't work on CPU**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input shape must be [mini_batch, time_sequence, embedding]\n",
    "def cudnn_stack_bilstm(input, num_stacked, dim_hidden, timesteps, dropout_rate, name):\n",
    "    # Define LSTMs\n",
    "    bilstmA = tf.contrib.cudnn_rnn.CudnnLSTM(num_stacked, dim_hidden, dropout=dropout_rate, name=\"forward_lstm\")\n",
    "    bilstmB = tf.contrib.cudnn_rnn.CudnnLSTM(num_stacked, dim_hidden, dropout=dropout_rate, name=\"backward_lstm\")\n",
    "    # Define Computation Graph\n",
    "    rnninput = tf.transpose(input, perm=[1,0,2])\n",
    "    rnnoutputA,_ = bilstmA(rnninput)\n",
    "    rnnoutputB,_ = bilstmB(rnninput[::-1])\n",
    "    # 各word毎のhidden representationが欲しい時\n",
    "    rnnoutput = tf.concat([rnnoutputA,rnnoutputB[::-1]],axis=2)\n",
    "    rnnoutput = tf.transpose(rnnoutput,perm=[1,0,2])\n",
    "    # 最初と最後のhidden representationを取ってくる\n",
    "    # rnnoutput_y = tf.concat([rnnoutputA,rnnoutputB],axis=2)[-1] # shape=[timestep, minibatch, emb]\n",
    "    return rnnoutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, let's use cudnn compagtible lstm now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input shape must be [mini_batch, time_sequence, embedding]\n",
    "def cudnn_cp_stack_bilstm(input, num_stacked, dim_hidden, timesteps, dropout, name):\n",
    "    # Unstack to expand input to a list of 'timesteps' tensors of shape (mini_batch, embedding)\n",
    "    input = tf.unstack(input, timesteps, 1)\n",
    "    # Define lstm cells with tensorflow\n",
    "    lstm_fw_cell = [tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell(dim_hidden, reuse=tf.AUTO_REUSE) for i in range(num_stacked)]\n",
    "    lstm_bw_cell = [tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell(dim_hidden, reuse=tf.AUTO_REUSE) for i in range(num_stacked)]\n",
    "    # Define computation graph of LSTMs\n",
    "    outputs, _, _ = tf.contrib.rnn.stack_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, input, dtype=tf.float32)\n",
    "    # Re-stack at axis of the timesteps.\n",
    "    outputs=tf.stack(outputs,axis=1)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use *cudnn_cp_stack_bilstm* now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 80)\n",
      "(?, 16, 80)\n"
     ]
    }
   ],
   "source": [
    "lstm_units=40\n",
    "hid_q = cudnn_cp_stack_bilstm(emb_q, 1, lstm_units, hp.maxlen, 0.5, \"q\")\n",
    "hid_a = cudnn_cp_stack_bilstm(emb_a, 1, lstm_units, hp.maxlen, 0.5, \"a\")\n",
    "print(hid_q.shape)\n",
    "print(hid_a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last dimension become twice of LSTM hidden units because of the concatenation of forward and backward LSTMs.\n",
    "Extract last and forward values of forward and backward LSTMs for a sentence representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 80)\n",
      "(?, 80)\n"
     ]
    }
   ],
   "source": [
    "rep_q = tf.concat([hid_q[:,-1,0:lstm_units+1],hid_q[:,0,lstm_units+1:]], axis=-1)\n",
    "rep_a = tf.concat([hid_a[:,-1,0:lstm_units+1],hid_a[:,0,lstm_units+1:]], axis=-1)\n",
    "print(rep_q.shape)\n",
    "print(rep_a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate question and answer representation and input to a MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 2)\n"
     ]
    }
   ],
   "source": [
    "rep = tf.concat([rep_q,rep_q], axis=-1)\n",
    "hl = tf.layers.dense(rep, hp.hidden_units, activation=tf.nn.relu)\n",
    "hl = tf.nn.dropout(hl,keep_prob=(0.1))\n",
    "logits = tf.layers.dense(hl, 2)   # binary classification\n",
    "prob = tf.nn.softmax(logits)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calc loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = tf.placeholder(tf.int32, shape=(None))   # binary class labels\n",
    "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\n",
    "mean_loss = tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tips:** *sparse_softmax_cross_entropy_with_logits* requires index of labels while *softmax_cross_entropy_with_logits(_v2)* requires distributions of labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=hp.lr, beta1=0.9, beta2=0.98, epsilon=1e-8)\n",
    "train_op = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train neural network with very simple datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[array([[1, 4, 5, 8, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 2, 6, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), array([[ 1,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 9, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])], [array([[ 1,  4,  5,  5,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 1,  4, 10,  5,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]), array([[10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])], [array([[4, 4, 5, 6, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [9, 4, 2, 6, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), array([[ 1,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 5, 10,  4, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])], [array([[ 4,  4,  5,  8,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 1,  4, 10,  5,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]), array([[1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])]]\n",
      "[array([0, 1]), array([1, 1]), array([0, 1]), array([0, 0])]\n",
      "Tensor(\"Placeholder_2:0\", dtype=int32)\n",
      "*** Epoch 1 ***\n",
      "Epoch: 1, Mini-batch: 0, loss: 0.625096\n",
      "Epoch: 1, Mini-batch: 1, loss: 0.691718\n",
      "Epoch: 1, Mini-batch: 2, loss: 0.693148\n",
      "Epoch: 1, Mini-batch: 3, loss: 0.716520\n",
      "*** Epoch 2 ***\n",
      "Epoch: 2, Mini-batch: 0, loss: 0.693148\n",
      "Epoch: 2, Mini-batch: 1, loss: 0.691713\n",
      "Epoch: 2, Mini-batch: 2, loss: 0.693148\n",
      "Epoch: 2, Mini-batch: 3, loss: 0.694600\n",
      "*** Epoch 3 ***\n",
      "Epoch: 3, Mini-batch: 0, loss: 0.693148\n",
      "Epoch: 3, Mini-batch: 1, loss: 0.772865\n",
      "Epoch: 3, Mini-batch: 2, loss: 0.599566\n",
      "Epoch: 3, Mini-batch: 3, loss: 0.694602\n",
      "*** Epoch 4 ***\n",
      "Epoch: 4, Mini-batch: 0, loss: 0.693148\n",
      "Epoch: 4, Mini-batch: 1, loss: 0.703488\n",
      "Epoch: 4, Mini-batch: 2, loss: 0.700354\n",
      "Epoch: 4, Mini-batch: 3, loss: 0.676168\n",
      "*** Epoch 5 ***\n",
      "Epoch: 5, Mini-batch: 0, loss: 0.693148\n",
      "Epoch: 5, Mini-batch: 1, loss: 0.673121\n",
      "Epoch: 5, Mini-batch: 2, loss: 0.590279\n",
      "Epoch: 5, Mini-batch: 3, loss: 0.694603\n",
      "*** Epoch 6 ***\n",
      "Epoch: 6, Mini-batch: 0, loss: 0.693148\n",
      "Epoch: 6, Mini-batch: 1, loss: 0.691706\n",
      "Epoch: 6, Mini-batch: 2, loss: 0.639931\n",
      "Epoch: 6, Mini-batch: 3, loss: 0.694610\n",
      "*** Epoch 7 ***\n",
      "Epoch: 7, Mini-batch: 0, loss: 0.693148\n",
      "Epoch: 7, Mini-batch: 1, loss: 0.678612\n",
      "Epoch: 7, Mini-batch: 2, loss: 0.693148\n",
      "Epoch: 7, Mini-batch: 3, loss: 0.684746\n",
      "*** Epoch 8 ***\n",
      "Epoch: 8, Mini-batch: 0, loss: 0.693148\n",
      "Epoch: 8, Mini-batch: 1, loss: 0.691689\n",
      "Epoch: 8, Mini-batch: 2, loss: 0.693148\n",
      "Epoch: 8, Mini-batch: 3, loss: 0.641229\n",
      "*** Epoch 9 ***\n",
      "Epoch: 9, Mini-batch: 0, loss: 0.693148\n",
      "Epoch: 9, Mini-batch: 1, loss: 0.691682\n",
      "Epoch: 9, Mini-batch: 2, loss: 0.693148\n",
      "Epoch: 9, Mini-batch: 3, loss: 0.694632\n",
      "*** Epoch 10 ***\n",
      "Epoch: 10, Mini-batch: 0, loss: 0.693148\n",
      "Epoch: 10, Mini-batch: 1, loss: 0.691677\n",
      "Epoch: 10, Mini-batch: 2, loss: 0.674164\n",
      "Epoch: 10, Mini-batch: 3, loss: 0.701974\n",
      "*** Epoch 11 ***\n",
      "Epoch: 11, Mini-batch: 0, loss: 0.693148\n",
      "Epoch: 11, Mini-batch: 1, loss: 0.691674\n",
      "Epoch: 11, Mini-batch: 2, loss: 0.542992\n",
      "Epoch: 11, Mini-batch: 3, loss: 0.694641\n",
      "*** Epoch 12 ***\n",
      "Epoch: 12, Mini-batch: 0, loss: 0.693148\n",
      "Epoch: 12, Mini-batch: 1, loss: 0.691665\n",
      "Epoch: 12, Mini-batch: 2, loss: 0.680526\n",
      "Epoch: 12, Mini-batch: 3, loss: 0.694651\n",
      "*** Epoch 13 ***\n",
      "Epoch: 13, Mini-batch: 0, loss: 0.693148\n",
      "Epoch: 13, Mini-batch: 1, loss: 0.682471\n",
      "Epoch: 13, Mini-batch: 2, loss: 0.693148\n",
      "Epoch: 13, Mini-batch: 3, loss: 0.675221\n",
      "*** Epoch 14 ***\n",
      "Epoch: 14, Mini-batch: 0, loss: 0.693148\n",
      "Epoch: 14, Mini-batch: 1, loss: 0.691652\n",
      "Epoch: 14, Mini-batch: 2, loss: 0.684908\n",
      "Epoch: 14, Mini-batch: 3, loss: 0.607520\n",
      "*** Epoch 15 ***\n",
      "Epoch: 15, Mini-batch: 0, loss: 0.703318\n",
      "Epoch: 15, Mini-batch: 1, loss: 0.682344\n",
      "Epoch: 15, Mini-batch: 2, loss: 0.684764\n",
      "Epoch: 15, Mini-batch: 3, loss: 0.694667\n",
      "*** Epoch 16 ***\n",
      "Epoch: 16, Mini-batch: 0, loss: 0.693148\n",
      "Epoch: 16, Mini-batch: 1, loss: 0.691643\n",
      "Epoch: 16, Mini-batch: 2, loss: 0.693655\n",
      "Epoch: 16, Mini-batch: 3, loss: 0.533547\n",
      "*** Epoch 17 ***\n",
      "Epoch: 17, Mini-batch: 0, loss: 0.693148\n",
      "Epoch: 17, Mini-batch: 1, loss: 0.709016\n",
      "Epoch: 17, Mini-batch: 2, loss: 0.693148\n",
      "Epoch: 17, Mini-batch: 3, loss: 0.694682\n",
      "*** Epoch 18 ***\n",
      "Epoch: 18, Mini-batch: 0, loss: 0.669771\n",
      "Epoch: 18, Mini-batch: 1, loss: 0.675851\n",
      "Epoch: 18, Mini-batch: 2, loss: 0.564034\n",
      "Epoch: 18, Mini-batch: 3, loss: 0.609349\n",
      "*** Epoch 19 ***\n",
      "Epoch: 19, Mini-batch: 0, loss: 0.671864\n",
      "Epoch: 19, Mini-batch: 1, loss: 0.682390\n",
      "Epoch: 19, Mini-batch: 2, loss: 0.516161\n",
      "Epoch: 19, Mini-batch: 3, loss: 0.576612\n",
      "*** Epoch 20 ***\n",
      "Epoch: 20, Mini-batch: 0, loss: 0.693148\n",
      "Epoch: 20, Mini-batch: 1, loss: 0.796739\n",
      "Epoch: 20, Mini-batch: 2, loss: 0.462608\n",
      "Epoch: 20, Mini-batch: 3, loss: 0.694720\n",
      "*** Epoch 21 ***\n",
      "Epoch: 21, Mini-batch: 0, loss: 0.693148\n",
      "Epoch: 21, Mini-batch: 1, loss: 0.653406\n",
      "Epoch: 21, Mini-batch: 2, loss: 0.514170\n",
      "Epoch: 21, Mini-batch: 3, loss: 0.610371\n",
      "*** Epoch 22 ***\n",
      "Epoch: 22, Mini-batch: 0, loss: 0.693148\n",
      "Epoch: 22, Mini-batch: 1, loss: 0.681438\n",
      "Epoch: 22, Mini-batch: 2, loss: 0.707460\n",
      "Epoch: 22, Mini-batch: 3, loss: 0.715535\n",
      "*** Epoch 23 ***\n",
      "Epoch: 23, Mini-batch: 0, loss: 0.714000\n",
      "Epoch: 23, Mini-batch: 1, loss: 0.700873\n",
      "Epoch: 23, Mini-batch: 2, loss: 0.564100\n",
      "Epoch: 23, Mini-batch: 3, loss: 0.694777\n",
      "*** Epoch 24 ***\n",
      "Epoch: 24, Mini-batch: 0, loss: 0.705270\n",
      "Epoch: 24, Mini-batch: 1, loss: 0.717748\n",
      "Epoch: 24, Mini-batch: 2, loss: 0.683595\n",
      "Epoch: 24, Mini-batch: 3, loss: 0.682346\n",
      "*** Epoch 25 ***\n",
      "Epoch: 25, Mini-batch: 0, loss: 0.693148\n",
      "Epoch: 25, Mini-batch: 1, loss: 0.661255\n",
      "Epoch: 25, Mini-batch: 2, loss: 0.693148\n",
      "Epoch: 25, Mini-batch: 3, loss: 0.694806\n",
      "*** Epoch 26 ***\n",
      "Epoch: 26, Mini-batch: 0, loss: 0.705166\n",
      "Epoch: 26, Mini-batch: 1, loss: 0.691503\n",
      "Epoch: 26, Mini-batch: 2, loss: 0.580717\n",
      "Epoch: 26, Mini-batch: 3, loss: 0.591057\n",
      "*** Epoch 27 ***\n",
      "Epoch: 27, Mini-batch: 0, loss: 0.651303\n",
      "Epoch: 27, Mini-batch: 1, loss: 0.691500\n",
      "Epoch: 27, Mini-batch: 2, loss: 0.565922\n",
      "Epoch: 27, Mini-batch: 3, loss: 0.694815\n",
      "*** Epoch 28 ***\n",
      "Epoch: 28, Mini-batch: 0, loss: 0.700784\n",
      "Epoch: 28, Mini-batch: 1, loss: 0.691494\n",
      "Epoch: 28, Mini-batch: 2, loss: 0.576417\n",
      "Epoch: 28, Mini-batch: 3, loss: 0.549662\n",
      "*** Epoch 29 ***\n",
      "Epoch: 29, Mini-batch: 0, loss: 0.732381\n",
      "Epoch: 29, Mini-batch: 1, loss: 0.691489\n",
      "Epoch: 29, Mini-batch: 2, loss: 0.693149\n",
      "Epoch: 29, Mini-batch: 3, loss: 0.694825\n",
      "*** Epoch 30 ***\n",
      "Epoch: 30, Mini-batch: 0, loss: 0.693149\n",
      "Epoch: 30, Mini-batch: 1, loss: 0.691485\n",
      "Epoch: 30, Mini-batch: 2, loss: 0.693149\n",
      "Epoch: 30, Mini-batch: 3, loss: 0.465732\n",
      "*** Epoch 31 ***\n",
      "Epoch: 31, Mini-batch: 0, loss: 0.693149\n",
      "Epoch: 31, Mini-batch: 1, loss: 0.654248\n",
      "Epoch: 31, Mini-batch: 2, loss: 0.693149\n",
      "Epoch: 31, Mini-batch: 3, loss: 0.694842\n",
      "*** Epoch 32 ***\n",
      "Epoch: 32, Mini-batch: 0, loss: 0.693149\n",
      "Epoch: 32, Mini-batch: 1, loss: 0.789703\n",
      "Epoch: 32, Mini-batch: 2, loss: 0.693149\n",
      "Epoch: 32, Mini-batch: 3, loss: 0.548644\n",
      "*** Epoch 33 ***\n",
      "Epoch: 33, Mini-batch: 0, loss: 0.639554\n",
      "Epoch: 33, Mini-batch: 1, loss: 0.682633\n",
      "Epoch: 33, Mini-batch: 2, loss: 0.678046\n",
      "Epoch: 33, Mini-batch: 3, loss: 0.694866\n",
      "*** Epoch 34 ***\n",
      "Epoch: 34, Mini-batch: 0, loss: 0.737930\n",
      "Epoch: 34, Mini-batch: 1, loss: 0.691441\n",
      "Epoch: 34, Mini-batch: 2, loss: 0.775321\n",
      "Epoch: 34, Mini-batch: 3, loss: 0.694875\n",
      "*** Epoch 35 ***\n",
      "Epoch: 35, Mini-batch: 0, loss: 0.693149\n",
      "Epoch: 35, Mini-batch: 1, loss: 0.691433\n",
      "Epoch: 35, Mini-batch: 2, loss: 0.693149\n",
      "Epoch: 35, Mini-batch: 3, loss: 0.694883\n",
      "*** Epoch 36 ***\n",
      "Epoch: 36, Mini-batch: 0, loss: 0.705016\n",
      "Epoch: 36, Mini-batch: 1, loss: 0.691427\n",
      "Epoch: 36, Mini-batch: 2, loss: 0.693149\n",
      "Epoch: 36, Mini-batch: 3, loss: 0.592406\n",
      "*** Epoch 37 ***\n",
      "Epoch: 37, Mini-batch: 0, loss: 0.693149\n",
      "Epoch: 37, Mini-batch: 1, loss: 0.691421\n",
      "Epoch: 37, Mini-batch: 2, loss: 0.693149\n",
      "Epoch: 37, Mini-batch: 3, loss: 0.624898\n",
      "*** Epoch 38 ***\n",
      "Epoch: 38, Mini-batch: 0, loss: 0.693149\n",
      "Epoch: 38, Mini-batch: 1, loss: 0.681150\n",
      "Epoch: 38, Mini-batch: 2, loss: 0.693149\n",
      "Epoch: 38, Mini-batch: 3, loss: 0.694903\n",
      "*** Epoch 39 ***\n",
      "Epoch: 39, Mini-batch: 0, loss: 0.693149\n",
      "Epoch: 39, Mini-batch: 1, loss: 0.745741\n",
      "Epoch: 39, Mini-batch: 2, loss: 0.669302\n",
      "Epoch: 39, Mini-batch: 3, loss: 0.595689\n",
      "*** Epoch 40 ***\n",
      "Epoch: 40, Mini-batch: 0, loss: 0.693149\n",
      "Epoch: 40, Mini-batch: 1, loss: 0.691396\n",
      "Epoch: 40, Mini-batch: 2, loss: 0.695771\n",
      "Epoch: 40, Mini-batch: 3, loss: 0.693708\n",
      "*** Epoch 41 ***\n",
      "Epoch: 41, Mini-batch: 0, loss: 0.642081\n",
      "Epoch: 41, Mini-batch: 1, loss: 0.691388\n",
      "Epoch: 41, Mini-batch: 2, loss: 0.699911\n",
      "Epoch: 41, Mini-batch: 3, loss: 0.694927\n",
      "*** Epoch 42 ***\n",
      "Epoch: 42, Mini-batch: 0, loss: 0.731883\n",
      "Epoch: 42, Mini-batch: 1, loss: 0.811847\n",
      "Epoch: 42, Mini-batch: 2, loss: 0.580769\n",
      "Epoch: 42, Mini-batch: 3, loss: 0.732302\n",
      "*** Epoch 43 ***\n",
      "Epoch: 43, Mini-batch: 0, loss: 0.693149\n",
      "Epoch: 43, Mini-batch: 1, loss: 0.691369\n",
      "Epoch: 43, Mini-batch: 2, loss: 0.693149\n",
      "Epoch: 43, Mini-batch: 3, loss: 0.694951\n",
      "*** Epoch 44 ***\n",
      "Epoch: 44, Mini-batch: 0, loss: 0.725507\n",
      "Epoch: 44, Mini-batch: 1, loss: 0.703956\n",
      "Epoch: 44, Mini-batch: 2, loss: 0.693149\n",
      "Epoch: 44, Mini-batch: 3, loss: 0.694960\n",
      "*** Epoch 45 ***\n",
      "Epoch: 45, Mini-batch: 0, loss: 0.693149\n",
      "Epoch: 45, Mini-batch: 1, loss: 0.691349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45, Mini-batch: 2, loss: 0.693149\n",
      "Epoch: 45, Mini-batch: 3, loss: 0.666187\n",
      "*** Epoch 46 ***\n",
      "Epoch: 46, Mini-batch: 0, loss: 0.650773\n",
      "Epoch: 46, Mini-batch: 1, loss: 0.691345\n",
      "Epoch: 46, Mini-batch: 2, loss: 0.693149\n",
      "Epoch: 46, Mini-batch: 3, loss: 0.667490\n",
      "*** Epoch 47 ***\n",
      "Epoch: 47, Mini-batch: 0, loss: 0.672226\n",
      "Epoch: 47, Mini-batch: 1, loss: 0.686588\n",
      "Epoch: 47, Mini-batch: 2, loss: 0.749033\n",
      "Epoch: 47, Mini-batch: 3, loss: 0.700319\n",
      "*** Epoch 48 ***\n",
      "Epoch: 48, Mini-batch: 0, loss: 0.649798\n",
      "Epoch: 48, Mini-batch: 1, loss: 0.789203\n",
      "Epoch: 48, Mini-batch: 2, loss: 0.693149\n",
      "Epoch: 48, Mini-batch: 3, loss: 0.564923\n",
      "*** Epoch 49 ***\n",
      "Epoch: 49, Mini-batch: 0, loss: 0.714190\n",
      "Epoch: 49, Mini-batch: 1, loss: 0.693150\n",
      "Epoch: 49, Mini-batch: 2, loss: 0.522639\n",
      "Epoch: 49, Mini-batch: 3, loss: 0.694991\n",
      "*** Epoch 50 ***\n",
      "Epoch: 50, Mini-batch: 0, loss: 0.703162\n",
      "Epoch: 50, Mini-batch: 1, loss: 0.682064\n",
      "Epoch: 50, Mini-batch: 2, loss: 0.684590\n",
      "Epoch: 50, Mini-batch: 3, loss: 0.695007\n",
      "*** Epoch 51 ***\n",
      "Epoch: 51, Mini-batch: 0, loss: 0.671495\n",
      "Epoch: 51, Mini-batch: 1, loss: 0.670432\n",
      "Epoch: 51, Mini-batch: 2, loss: 0.693448\n",
      "Epoch: 51, Mini-batch: 3, loss: 0.695013\n",
      "*** Epoch 52 ***\n",
      "Epoch: 52, Mini-batch: 0, loss: 0.635423\n",
      "Epoch: 52, Mini-batch: 1, loss: 0.691296\n",
      "Epoch: 52, Mini-batch: 2, loss: 0.693149\n",
      "Epoch: 52, Mini-batch: 3, loss: 0.662204\n",
      "*** Epoch 53 ***\n",
      "Epoch: 53, Mini-batch: 0, loss: 0.644318\n",
      "Epoch: 53, Mini-batch: 1, loss: 0.687512\n",
      "Epoch: 53, Mini-batch: 2, loss: 0.693149\n",
      "Epoch: 53, Mini-batch: 3, loss: 0.699896\n",
      "*** Epoch 54 ***\n",
      "Epoch: 54, Mini-batch: 0, loss: 0.693149\n",
      "Epoch: 54, Mini-batch: 1, loss: 0.764654\n",
      "Epoch: 54, Mini-batch: 2, loss: 0.572139\n",
      "Epoch: 54, Mini-batch: 3, loss: 0.695034\n",
      "*** Epoch 55 ***\n",
      "Epoch: 55, Mini-batch: 0, loss: 0.693149\n",
      "Epoch: 55, Mini-batch: 1, loss: 0.742417\n",
      "Epoch: 55, Mini-batch: 2, loss: 0.541897\n",
      "Epoch: 55, Mini-batch: 3, loss: 0.704718\n",
      "*** Epoch 56 ***\n",
      "Epoch: 56, Mini-batch: 0, loss: 0.693149\n",
      "Epoch: 56, Mini-batch: 1, loss: 0.691268\n",
      "Epoch: 56, Mini-batch: 2, loss: 0.525919\n",
      "Epoch: 56, Mini-batch: 3, loss: 0.561211\n",
      "*** Epoch 57 ***\n",
      "Epoch: 57, Mini-batch: 0, loss: 0.680821\n",
      "Epoch: 57, Mini-batch: 1, loss: 0.691248\n",
      "Epoch: 57, Mini-batch: 2, loss: 0.693149\n",
      "Epoch: 57, Mini-batch: 3, loss: 0.705016\n",
      "*** Epoch 58 ***\n",
      "Epoch: 58, Mini-batch: 0, loss: 0.693149\n",
      "Epoch: 58, Mini-batch: 1, loss: 0.662791\n",
      "Epoch: 58, Mini-batch: 2, loss: 0.576097\n",
      "Epoch: 58, Mini-batch: 3, loss: 0.695086\n",
      "*** Epoch 59 ***\n",
      "Epoch: 59, Mini-batch: 0, loss: 0.690882\n",
      "Epoch: 59, Mini-batch: 1, loss: 0.691227\n",
      "Epoch: 59, Mini-batch: 2, loss: 0.693149\n",
      "Epoch: 59, Mini-batch: 3, loss: 0.695086\n",
      "*** Epoch 60 ***\n",
      "Epoch: 60, Mini-batch: 0, loss: 0.647130\n",
      "Epoch: 60, Mini-batch: 1, loss: 0.695410\n",
      "Epoch: 60, Mini-batch: 2, loss: 0.683802\n",
      "Epoch: 60, Mini-batch: 3, loss: 0.695088\n",
      "*** Epoch 61 ***\n",
      "Epoch: 61, Mini-batch: 0, loss: 0.693149\n",
      "Epoch: 61, Mini-batch: 1, loss: 0.647973\n",
      "Epoch: 61, Mini-batch: 2, loss: 0.559906\n",
      "Epoch: 61, Mini-batch: 3, loss: 0.695090\n",
      "*** Epoch 62 ***\n",
      "Epoch: 62, Mini-batch: 0, loss: 0.693149\n",
      "Epoch: 62, Mini-batch: 1, loss: 0.691220\n",
      "Epoch: 62, Mini-batch: 2, loss: 0.520169\n",
      "Epoch: 62, Mini-batch: 3, loss: 0.695098\n",
      "*** Epoch 63 ***\n",
      "Epoch: 63, Mini-batch: 0, loss: 0.693149\n",
      "Epoch: 63, Mini-batch: 1, loss: 0.691208\n",
      "Epoch: 63, Mini-batch: 2, loss: 0.693149\n",
      "Epoch: 63, Mini-batch: 3, loss: 0.705439\n",
      "*** Epoch 64 ***\n",
      "Epoch: 64, Mini-batch: 0, loss: 0.693149\n",
      "Epoch: 64, Mini-batch: 1, loss: 0.748848\n",
      "Epoch: 64, Mini-batch: 2, loss: 0.710288\n",
      "Epoch: 64, Mini-batch: 3, loss: 0.695119\n",
      "*** Epoch 65 ***\n",
      "Epoch: 65, Mini-batch: 0, loss: 0.693149\n",
      "Epoch: 65, Mini-batch: 1, loss: 0.691190\n",
      "Epoch: 65, Mini-batch: 2, loss: 0.693149\n",
      "Epoch: 65, Mini-batch: 3, loss: 0.695126\n",
      "*** Epoch 66 ***\n",
      "Epoch: 66, Mini-batch: 0, loss: 0.691554\n",
      "Epoch: 66, Mini-batch: 1, loss: 0.683254\n",
      "Epoch: 66, Mini-batch: 2, loss: 0.688847\n",
      "Epoch: 66, Mini-batch: 3, loss: 0.695130\n",
      "*** Epoch 67 ***\n",
      "Epoch: 67, Mini-batch: 0, loss: 0.617535\n",
      "Epoch: 67, Mini-batch: 1, loss: 0.691183\n",
      "Epoch: 67, Mini-batch: 2, loss: 0.693149\n",
      "Epoch: 67, Mini-batch: 3, loss: 0.695128\n",
      "*** Epoch 68 ***\n",
      "Epoch: 68, Mini-batch: 0, loss: 0.693149\n",
      "Epoch: 68, Mini-batch: 1, loss: 0.737185\n",
      "Epoch: 68, Mini-batch: 2, loss: 0.554960\n",
      "Epoch: 68, Mini-batch: 3, loss: 0.526704\n",
      "*** Epoch 69 ***\n",
      "Epoch: 69, Mini-batch: 0, loss: 0.693149\n",
      "Epoch: 69, Mini-batch: 1, loss: 0.691187\n",
      "Epoch: 69, Mini-batch: 2, loss: 0.693149\n",
      "Epoch: 69, Mini-batch: 3, loss: 0.616891\n",
      "*** Epoch 70 ***\n",
      "Epoch: 70, Mini-batch: 0, loss: 0.693149\n",
      "Epoch: 70, Mini-batch: 1, loss: 0.691182\n",
      "Epoch: 70, Mini-batch: 2, loss: 0.682725\n",
      "Epoch: 70, Mini-batch: 3, loss: 0.695134\n",
      "*** Epoch 71 ***\n",
      "Epoch: 71, Mini-batch: 0, loss: 0.704439\n",
      "Epoch: 71, Mini-batch: 1, loss: 0.691177\n",
      "Epoch: 71, Mini-batch: 2, loss: 0.546428\n",
      "Epoch: 71, Mini-batch: 3, loss: 0.695139\n",
      "*** Epoch 72 ***\n",
      "Epoch: 72, Mini-batch: 0, loss: 0.693149\n",
      "Epoch: 72, Mini-batch: 1, loss: 0.638344\n",
      "Epoch: 72, Mini-batch: 2, loss: 0.693207\n",
      "Epoch: 72, Mini-batch: 3, loss: 0.695147\n",
      "*** Epoch 73 ***\n",
      "Epoch: 73, Mini-batch: 0, loss: 0.726356\n",
      "Epoch: 73, Mini-batch: 1, loss: 0.691165\n",
      "Epoch: 73, Mini-batch: 2, loss: 0.693149\n",
      "Epoch: 73, Mini-batch: 3, loss: 0.673985\n",
      "*** Epoch 74 ***\n",
      "Epoch: 74, Mini-batch: 0, loss: 0.679348\n",
      "Epoch: 74, Mini-batch: 1, loss: 0.691163\n",
      "Epoch: 74, Mini-batch: 2, loss: 0.693149\n",
      "Epoch: 74, Mini-batch: 3, loss: 0.706087\n",
      "*** Epoch 75 ***\n",
      "Epoch: 75, Mini-batch: 0, loss: 0.679243\n",
      "Epoch: 75, Mini-batch: 1, loss: 0.691163\n",
      "Epoch: 75, Mini-batch: 2, loss: 0.693149\n",
      "Epoch: 75, Mini-batch: 3, loss: 0.524894\n",
      "*** Epoch 76 ***\n",
      "Epoch: 76, Mini-batch: 0, loss: 0.662192\n",
      "Epoch: 76, Mini-batch: 1, loss: 0.710913\n",
      "Epoch: 76, Mini-batch: 2, loss: 0.693149\n",
      "Epoch: 76, Mini-batch: 3, loss: 0.695160\n",
      "*** Epoch 77 ***\n",
      "Epoch: 77, Mini-batch: 0, loss: 0.693149\n",
      "Epoch: 77, Mini-batch: 1, loss: 0.691149\n",
      "Epoch: 77, Mini-batch: 2, loss: 0.693149\n",
      "Epoch: 77, Mini-batch: 3, loss: 0.695167\n",
      "*** Epoch 78 ***\n",
      "Epoch: 78, Mini-batch: 0, loss: 0.740040\n",
      "Epoch: 78, Mini-batch: 1, loss: 0.691144\n",
      "Epoch: 78, Mini-batch: 2, loss: 0.693149\n",
      "Epoch: 78, Mini-batch: 3, loss: 0.537136\n",
      "*** Epoch 79 ***\n",
      "Epoch: 79, Mini-batch: 0, loss: 0.678914\n",
      "Epoch: 79, Mini-batch: 1, loss: 0.691138\n",
      "Epoch: 79, Mini-batch: 2, loss: 0.564641\n",
      "Epoch: 79, Mini-batch: 3, loss: 0.695178\n",
      "*** Epoch 80 ***\n",
      "Epoch: 80, Mini-batch: 0, loss: 0.693149\n",
      "Epoch: 80, Mini-batch: 1, loss: 0.701498\n",
      "Epoch: 80, Mini-batch: 2, loss: 0.670126\n",
      "Epoch: 80, Mini-batch: 3, loss: 0.557707\n",
      "*** Epoch 81 ***\n",
      "Epoch: 81, Mini-batch: 0, loss: 0.693149\n",
      "Epoch: 81, Mini-batch: 1, loss: 0.682752\n",
      "Epoch: 81, Mini-batch: 2, loss: 0.400845\n",
      "Epoch: 81, Mini-batch: 3, loss: 0.695188\n",
      "*** Epoch 82 ***\n",
      "Epoch: 82, Mini-batch: 0, loss: 0.624536\n",
      "Epoch: 82, Mini-batch: 1, loss: 0.682079\n",
      "Epoch: 82, Mini-batch: 2, loss: 0.693149\n",
      "Epoch: 82, Mini-batch: 3, loss: 0.695194\n",
      "*** Epoch 83 ***\n",
      "Epoch: 83, Mini-batch: 0, loss: 0.736397\n",
      "Epoch: 83, Mini-batch: 1, loss: 0.648690\n",
      "Epoch: 83, Mini-batch: 2, loss: 0.693149\n",
      "Epoch: 83, Mini-batch: 3, loss: 0.375538\n",
      "*** Epoch 84 ***\n",
      "Epoch: 84, Mini-batch: 0, loss: 0.693149\n",
      "Epoch: 84, Mini-batch: 1, loss: 0.731444\n",
      "Epoch: 84, Mini-batch: 2, loss: 0.681493\n",
      "Epoch: 84, Mini-batch: 3, loss: 0.505118\n",
      "*** Epoch 85 ***\n",
      "Epoch: 85, Mini-batch: 0, loss: 0.693149\n",
      "Epoch: 85, Mini-batch: 1, loss: 0.691088\n",
      "Epoch: 85, Mini-batch: 2, loss: 0.693149\n",
      "Epoch: 85, Mini-batch: 3, loss: 0.746154\n",
      "*** Epoch 86 ***\n",
      "Epoch: 86, Mini-batch: 0, loss: 0.723773\n",
      "Epoch: 86, Mini-batch: 1, loss: 0.691071\n",
      "Epoch: 86, Mini-batch: 2, loss: 0.693149\n",
      "Epoch: 86, Mini-batch: 3, loss: 0.707635\n",
      "*** Epoch 87 ***\n",
      "Epoch: 87, Mini-batch: 0, loss: 0.693149\n",
      "Epoch: 87, Mini-batch: 1, loss: 0.740512\n",
      "Epoch: 87, Mini-batch: 2, loss: 0.693149\n",
      "Epoch: 87, Mini-batch: 3, loss: 0.524379\n",
      "*** Epoch 88 ***\n",
      "Epoch: 88, Mini-batch: 0, loss: 0.669937\n",
      "Epoch: 88, Mini-batch: 1, loss: 0.679157\n",
      "Epoch: 88, Mini-batch: 2, loss: 0.693149\n",
      "Epoch: 88, Mini-batch: 3, loss: 0.695270\n",
      "*** Epoch 89 ***\n",
      "Epoch: 89, Mini-batch: 0, loss: 0.677281\n",
      "Epoch: 89, Mini-batch: 1, loss: 0.708244\n",
      "Epoch: 89, Mini-batch: 2, loss: 0.664765\n",
      "Epoch: 89, Mini-batch: 3, loss: 0.649752\n",
      "*** Epoch 90 ***\n",
      "Epoch: 90, Mini-batch: 0, loss: 0.659405\n",
      "Epoch: 90, Mini-batch: 1, loss: 0.679409\n",
      "Epoch: 90, Mini-batch: 2, loss: 0.699184\n",
      "Epoch: 90, Mini-batch: 3, loss: 0.700682\n",
      "*** Epoch 91 ***\n",
      "Epoch: 91, Mini-batch: 0, loss: 0.693149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 91, Mini-batch: 1, loss: 0.679479\n",
      "Epoch: 91, Mini-batch: 2, loss: 0.506374\n",
      "Epoch: 91, Mini-batch: 3, loss: 0.463973\n",
      "*** Epoch 92 ***\n",
      "Epoch: 92, Mini-batch: 0, loss: 0.675094\n",
      "Epoch: 92, Mini-batch: 1, loss: 0.690999\n",
      "Epoch: 92, Mini-batch: 2, loss: 0.693150\n",
      "Epoch: 92, Mini-batch: 3, loss: 0.541014\n",
      "*** Epoch 93 ***\n",
      "Epoch: 93, Mini-batch: 0, loss: 0.676507\n",
      "Epoch: 93, Mini-batch: 1, loss: 0.690973\n",
      "Epoch: 93, Mini-batch: 2, loss: 0.497589\n",
      "Epoch: 93, Mini-batch: 3, loss: 0.694428\n",
      "*** Epoch 94 ***\n",
      "Epoch: 94, Mini-batch: 0, loss: 0.622995\n",
      "Epoch: 94, Mini-batch: 1, loss: 0.692292\n",
      "Epoch: 94, Mini-batch: 2, loss: 0.695249\n",
      "Epoch: 94, Mini-batch: 3, loss: 0.693867\n",
      "*** Epoch 95 ***\n",
      "Epoch: 95, Mini-batch: 0, loss: 0.693150\n",
      "Epoch: 95, Mini-batch: 1, loss: 0.809106\n",
      "Epoch: 95, Mini-batch: 2, loss: 0.693150\n",
      "Epoch: 95, Mini-batch: 3, loss: 0.695411\n",
      "*** Epoch 96 ***\n",
      "Epoch: 96, Mini-batch: 0, loss: 0.693150\n",
      "Epoch: 96, Mini-batch: 1, loss: 0.678264\n",
      "Epoch: 96, Mini-batch: 2, loss: 0.693150\n",
      "Epoch: 96, Mini-batch: 3, loss: 0.695428\n",
      "*** Epoch 97 ***\n",
      "Epoch: 97, Mini-batch: 0, loss: 0.693150\n",
      "Epoch: 97, Mini-batch: 1, loss: 0.690881\n",
      "Epoch: 97, Mini-batch: 2, loss: 0.693150\n",
      "Epoch: 97, Mini-batch: 3, loss: 0.456302\n",
      "*** Epoch 98 ***\n",
      "Epoch: 98, Mini-batch: 0, loss: 0.713204\n",
      "Epoch: 98, Mini-batch: 1, loss: 0.759921\n",
      "Epoch: 98, Mini-batch: 2, loss: 0.607903\n",
      "Epoch: 98, Mini-batch: 3, loss: 0.695459\n",
      "*** Epoch 99 ***\n",
      "Epoch: 99, Mini-batch: 0, loss: 0.693150\n",
      "Epoch: 99, Mini-batch: 1, loss: 0.690850\n",
      "Epoch: 99, Mini-batch: 2, loss: 0.693150\n",
      "Epoch: 99, Mini-batch: 3, loss: 0.458265\n",
      "*** Epoch 100 ***\n",
      "Epoch: 100, Mini-batch: 0, loss: 0.641044\n",
      "Epoch: 100, Mini-batch: 1, loss: 0.672848\n",
      "Epoch: 100, Mini-batch: 2, loss: 0.541522\n",
      "Epoch: 100, Mini-batch: 3, loss: 0.695487\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "datasets=[\n",
    "        [np.array([ [1,4,5,8,3]+[0 for _ in range(11)], [1,1,2,6,3]+[0 for _ in range(11)] ]),\n",
    "         np.array([ [1,2]+[0 for _ in range(14)],       [9,10]+[0 for _ in range(14)]])],\n",
    "        [np.array([ [1,4,5,5,3]+[0 for _ in range(11)], [1,4,10,5,3]+[0 for _ in range(11)] ]),\n",
    "         np.array([ [10]+[0 for _ in range(15)],        [10]+[0 for _ in range(15)]])],\n",
    "        [np.array([ [4,4,5,6,3]+[0 for _ in range(11)], [9,4,2,6,3]+[0 for _ in range(11)] ]),\n",
    "         np.array([ [1,2]+[0 for _ in range(14)],       [5,10,4,10]+[0 for _ in range(12)]])],\n",
    "        [np.array([ [4,4,5,8,3]+[0 for _ in range(11)], [1,4,10,5,3]+[0 for _ in range(11)] ]),\n",
    "         np.array([ [1,2]+[0 for _ in range(14)],        [1,2]+[0 for _ in range(14)]])],\n",
    "]\n",
    "labels=[np.array([0,1]), np.array([1,1]), np.array([0,1]), np.array([0,0])]\n",
    "print(datasets)\n",
    "print(labels)\n",
    "print(label)\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    print(\"*** Epoch %d ***\"%epoch)\n",
    "    for mb,(data,lab) in enumerate(zip(datasets,labels)):\n",
    "        feed_dict={\n",
    "                idx_q:data[0],\n",
    "                idx_a:data[1],\n",
    "                label:lab,\n",
    "                }\n",
    "        log_loss,log_train_op = sess.run([mean_loss,train_op],feed_dict=feed_dict)\n",
    "        print(\"Epoch: %d, Mini-batch: %d, loss: %f\"%(epoch,mb,log_loss))\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tips:** *sess.run(fetches,feed_dict)* is a function to execute real computations and evaluate (get) tensors specified as *fetches*, given the tensors by *feed_dict*. This can be used for training, testing and other computation operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with your net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability: [array([[0.49883467, 0.50116533],\n",
      "       [0.49883467, 0.50116533]], dtype=float32)]\n",
      "Argmax: [[1 1]]\n",
      "Probability: [array([[0.4894198 , 0.51058024],\n",
      "       [0.49883467, 0.50116533]], dtype=float32)]\n",
      "Argmax: [[1 1]]\n",
      "Probability: [array([[0.7456393 , 0.25436068],\n",
      "       [0.49883467, 0.50116533]], dtype=float32)]\n",
      "Argmax: [[0 1]]\n",
      "Probability: [array([[0.49883467, 0.50116533],\n",
      "       [0.56734097, 0.4326591 ]], dtype=float32)]\n",
      "Argmax: [[1 0]]\n"
     ]
    }
   ],
   "source": [
    "for data,lab in zip(datasets,labels):\n",
    "    feed_dict={\n",
    "            idx_q:data[0],\n",
    "            idx_a:data[1],\n",
    "            label:lab,\n",
    "            }\n",
    "    log_prob = sess.run([prob],feed_dict=feed_dict)\n",
    "    log_class = np.argmax(log_prob,axis=-1)\n",
    "    print(\"Probability: \"+str(log_prob))\n",
    "    print(\"Argmax: \"+str(log_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, this dataset is too tiny to train this network..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign default values to the lookup table.\n",
    "Use sess.run() after creating a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.6176402 ,  1.346004  , -0.45719635, ..., -0.17219022,\n",
       "         -2.1217787 , -1.8355305 ],\n",
       "        [ 0.24177887, -0.5585114 ,  0.87253124, ...,  0.3342192 ,\n",
       "         -1.1395321 ,  2.7439766 ],\n",
       "        [-0.14339474, -2.2741785 , -0.01487757, ...,  0.170371  ,\n",
       "         -0.00736693, -0.79158056],\n",
       "        ...,\n",
       "        [ 0.43974823,  0.6796133 ,  0.432675  , ...,  0.61743784,\n",
       "          1.3885504 , -0.8581818 ],\n",
       "        [ 0.62102765, -0.31394094,  0.73098993, ...,  0.7072901 ,\n",
       "         -0.15805143, -0.47703704],\n",
       "        [-1.2269344 ,  1.3161734 ,  2.357299  , ...,  0.9376848 ,\n",
       "         -1.6746929 , -0.5490803 ]], dtype=float32)]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define lookup table.\n",
    "embedding_shape=(vocab_size, num_units)\n",
    "emb_w = tf.Variable(tf.constant(0.0, shape=embedding_shape), name=\"emb_w\")\n",
    "embedding_placeholder_w = tf.placeholder(tf.float32, embedding_shape)\n",
    "embedding_init_w = emb_w.assign(embedding_placeholder_w)      # `assign` operation.\n",
    "# ...\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# Run assign computation graph.\n",
    "init_wmat=np.random.randn(*embedding_shape)                   # Of course, you can swap this with gensim vec. \n",
    "sess.run([embedding_init_w], feed_dict={embedding_placeholder_w: init_wmat})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a special line for tensorboard. You don't need to run below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.summary.writer.writer.FileWriter at 0x1244dfe48>"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.summary.FileWriter(logdir='./graph/', graph=tf.Graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kurita/repos/tensorflow-tutorial2018/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "\u001b[33mW0925 15:36:04.448749 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0925 15:36:04.448749 123145489252352 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0925 15:36:04.450213 Reloader tf_logging.py:120] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "\u001b[0mW0925 15:36:04.450213 123145489252352 tf_logging.py:120] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "\u001b[33mW0925 15:36:04.459388 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0925 15:36:04.459388 123145489252352 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0925 15:36:04.467729 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0925 15:36:04.467729 123145489252352 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "TensorBoard 1.10.0 at http://chako:6006 (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir='./graph/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access localhost:PORT_NUM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips\n",
    "This is the BasicLSTM of Tensorflow. You might see in many websites introducing tensorflow.<br>\n",
    "However, experts rarely use this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input shape must be [mini_batch, time_sequence, embedding]\n",
    "def simple_stack_bilstm(input, num_stacked, dim_hidden, timesteps, dropout, name):\n",
    "   # Unstack to expand input to a list of 'timesteps' tensors of shape (mini_batch, embedding)\n",
    "    input = tf.unstack(input, timesteps, 1)\n",
    "    # Define lstm cells with tensorflow\n",
    "    lstm_fw_cell = [tf.contrib.rnn.BasicLSTMCell(dim_hidden, forget_bias=1.0, reuse=tf.AUTO_REUSE, name=name+\"_f_\"+str(i)) for i in range(num_stacked)]\n",
    "    lstm_bw_cell = [tf.contrib.rnn.BasicLSTMCell(dim_hidden, forget_bias=1.0, reuse=tf.AUTO_REUSE, name=name+\"_b_\"+str(i)) for i in range(num_stacked)]\n",
    "    # Define computation graph of LSTMs\n",
    "    outputs, _, _ = tf.contrib.rnn.stack_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, input, dtype=tf.float32)\n",
    "    outputs=tf.stack(outputs,axis=1)\n",
    "    return outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
