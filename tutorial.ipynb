{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-excerse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the jupyter notebook! Before the tensorflow tutorial, how about excersing on jupyter notebook?<br>\n",
    "So this is the **introduction to jupyter notebook!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you can execute shell commands following by `!`,<br>\n",
    "otherwise your commands become ipython scripts.<br>\n",
    "You can *execute* them by `Ctrl+Enter` (or click `Run` buttom at the top bar).<br>\n",
    "`Ctrl+m l` to show line numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are using your default shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $SHELL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run more complex commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls;touch foobar;ls -la foobar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can edit the inside of the command by clicking. Edit the following incomplete command to remove *foobar* file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmfoobar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the commands are running, the state become \\[\\*\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!for iii in `seq 10 0`; do sleep 1; echo $iii; done; echo finished!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, you can edit textlines on this notebook by double-clicking.<br>\n",
    "If you want to insert a *text line*, click `+` bottom and select `Code` to `Markdown`. <br>\n",
    "After you edit a text line, just `Ctrl+Enter` to replace old lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also include links and pictures as Markdown format.\n",
    "![jupyter icon](http://jupyter.org/assets/main-logo.svg)\n",
    "How about creating [your own jupyter notebook](blank.ipynb)\\?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before processing, check you are using virtualenv python (/XXXXXX/tensorflow-tutorial2018/tensorflow/bin/python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, do not forget you are in *ipython*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def dice():\n",
    "    return random.randint(1,6)\n",
    "print(dice())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play with this *dice* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0]\n",
      "[187 166 175 158 146 168]\n",
      "Requirement already satisfied: matplotlib in ./tensorflow/lib/python3.6/site-packages (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.10.0 in ./tensorflow/lib/python3.6/site-packages (from matplotlib) (1.14.5)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in ./tensorflow/lib/python3.6/site-packages (from matplotlib) (2.7.3)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in ./tensorflow/lib/python3.6/site-packages (from matplotlib) (2.2.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./tensorflow/lib/python3.6/site-packages (from matplotlib) (1.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./tensorflow/lib/python3.6/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in ./tensorflow/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib) (1.11.0)\n",
      "Requirement already satisfied: setuptools in ./tensorflow/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib) (39.1.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11ea7ef98>]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "stat=np.zeros((6,),dtype=\"int64\")  # creating [0,0,0,0,0,0]\n",
    "print(stat)\n",
    "N=1000\n",
    "for i in range(N):\n",
    "    stat[dice()-1]+=1\n",
    "print(stat)\n",
    "\n",
    "# Drawing a graph\n",
    "!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.bar(np.arange(1,7),stat/N, alpha=0.5)    # draw a bar graph. alpha for transparency\n",
    "x = np.arange(1, 6, 0.1)\n",
    "y = x*0+1./6\n",
    "plt.plot(x,y, color=\"red\",linestyle=\"dotted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boring graph? Ok, draw a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1200c19e8>]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "N,M,BINS=1000,1000,50\n",
    "means=[]\n",
    "for i in range(N):\n",
    "    stat=[]\n",
    "    for j in range(M):\n",
    "        stat.append(dice())\n",
    "    means.append(np.mean(stat))\n",
    "# Drawing a histgram\n",
    "plt.hist(means, bins=BINS, alpha=0.5) # draw a histgram (unnormalized distribution, or counts per bin) of samples.\n",
    "\n",
    "# Central limit theorem (中心極限定理)\n",
    "x = np.arange(3, 4, 0.01)\n",
    "nu,sigma=3.5,np.sqrt(35/12./M)\n",
    "y = (1/np.sqrt(2*np.pi*(sigma**2)))*np.exp(-((x-nu)**2/(2*sigma**2))) *((np.max(means)-np.min(means))/BINS*N)\n",
    "plt.plot(x,y, color=\"red\",linestyle=\"dotted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So-called *Matlab-like*!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a trick to close and re-launch ipython kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's enter the tensorflow tutorial.\n",
    "Run [train.py](http://localhost:8888/edit/train.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code reading\n",
    "Let's see the code of Tensorflow.<br>\n",
    "(Note: In many tensorflow codes, tensorflow computation graph is defined in a python class, e.g. *class Graph()* in *train.py*. We do not use class for simplicity now.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a neural network on ipython.<br>\n",
    "The neural network consists of two bi-LSTMs and 2-layer MLP for binary classification.<br>\n",
    "The inputs are word index of pairs of question and answer sentences.<br>\n",
    "If the question and answer sentence is related, the label is 1, otherwise 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from data_load import get_batch_data, load_vocab\n",
    "from hyperparams import Hyperparams as hp\n",
    "from modules import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a command for reseting tensorflow graph.<br>\n",
    "If you change your neural network, run the following code and recreate neural network from the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define placeholders. Placeholders are containers for inputs of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16)\n",
      "(?, 16)\n"
     ]
    }
   ],
   "source": [
    "idx_q = tf.placeholder(tf.int32, shape=(None, hp.maxlen)) # Question sentence's word index\n",
    "idx_a = tf.placeholder(tf.int32, shape=(None, hp.maxlen)) # Answer sentence's word index\n",
    "print(idx_q.shape)\n",
    "print(idx_a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "Change index to embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 40)\n",
      "(?, 16, 40)\n"
     ]
    }
   ],
   "source": [
    "vocab_size, num_units=100,40\n",
    "# Define\n",
    "lookup_table = tf.get_variable('lookup_table',\n",
    "                                       dtype=tf.float32,\n",
    "                                       shape=[vocab_size, num_units],\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "# Lookup\n",
    "emb_q = tf.nn.embedding_lookup(lookup_table, idx_q)\n",
    "emb_a = tf.nn.embedding_lookup(lookup_table, idx_a)\n",
    "print(emb_q.shape)\n",
    "print(emb_a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will explain how to initialize *embedding* later.\n",
    "Let's define LSTMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "This is a simple rapper function for stacked bi-LSTM layers.<br>\n",
    "This works for both CPU and GPU.<br>\n",
    "https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/stack_bidirectional_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a cudnn implementation of stacked LSTM. Unfortunately, **this doesn't work on CPU**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input shape must be [mini_batch, time_sequence, embedding]\n",
    "def cudnn_stack_bilstm(input, num_stacked, dim_hidden, timesteps, dropout_rate, name):\n",
    "    # Define LSTMs\n",
    "    bilstmA = tf.contrib.cudnn_rnn.CudnnLSTM(num_stacked, dim_hidden, dropout=dropout_rate, name=\"forward_lstm\")\n",
    "    bilstmB = tf.contrib.cudnn_rnn.CudnnLSTM(num_stacked, dim_hidden, dropout=dropout_rate, name=\"backward_lstm\")\n",
    "    # Define Computation Graph\n",
    "    rnninput = tf.transpose(input, perm=[1,0,2])\n",
    "    rnnoutputA,_ = bilstmA(rnninput)\n",
    "    rnnoutputB,_ = bilstmB(rnninput[::-1])\n",
    "    # 各word毎のhidden representationが欲しい時\n",
    "    rnnoutput = tf.concat([rnnoutputA,rnnoutputB[::-1]],axis=2)\n",
    "    rnnoutput = tf.transpose(rnnoutput,perm=[1,0,2])\n",
    "    # 最初と最後のhidden representationを取ってくる\n",
    "    # rnnoutput_y = tf.concat([rnnoutputA,rnnoutputB],axis=2)[-1] # shape=[timestep, minibatch, emb]\n",
    "    return rnnoutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, let's use cudnn compagtible lstm for CPU training and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input shape must be [mini_batch, time_sequence, embedding]\n",
    "def cudnn_cp_stack_bilstm(input, num_stacked, dim_hidden, timesteps, dropout, name):\n",
    "    # Unstack to expand input to a list of 'timesteps' tensors of shape (mini_batch, embedding)\n",
    "    input = tf.unstack(input, timesteps, 1)\n",
    "    # Define lstm cells with tensorflow\n",
    "    lstm_fw_cell = [tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell(dim_hidden, reuse=tf.AUTO_REUSE) for i in range(num_stacked)]\n",
    "    lstm_bw_cell = [tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell(dim_hidden, reuse=tf.AUTO_REUSE) for i in range(num_stacked)]\n",
    "    # Define computation graph of LSTMs\n",
    "    outputs, _, _ = tf.contrib.rnn.stack_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, input, dtype=tf.float32)\n",
    "    # Re-stack at axis of the timesteps.\n",
    "    outputs=tf.stack(outputs,axis=1)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use *cudnn_cp_stack_bilstm* now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 80)\n",
      "(?, 16, 80)\n"
     ]
    }
   ],
   "source": [
    "lstm_units=40\n",
    "hid_q = cudnn_cp_stack_bilstm(emb_q, 1, lstm_units, hp.maxlen, 0.5, \"q\")\n",
    "hid_a = cudnn_cp_stack_bilstm(emb_a, 1, lstm_units, hp.maxlen, 0.5, \"a\")\n",
    "print(hid_q.shape)\n",
    "print(hid_a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last dimension become twice of LSTM hidden units because of the concatenation of forward and backward LSTMs.\n",
    "Extract last and forward values of forward and backward LSTMs for a sentence representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 80)\n",
      "(?, 80)\n"
     ]
    }
   ],
   "source": [
    "rep_q = tf.concat([hid_q[:,-1,0:lstm_units+1],hid_q[:,0,lstm_units+1:]], axis=-1)\n",
    "rep_a = tf.concat([hid_a[:,-1,0:lstm_units+1],hid_a[:,0,lstm_units+1:]], axis=-1)\n",
    "print(rep_q.shape)\n",
    "print(rep_a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate question and answer representation and input to a MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 2)\n"
     ]
    }
   ],
   "source": [
    "rep = tf.concat([rep_q,rep_q], axis=-1)\n",
    "hl = tf.layers.dense(rep, hp.hidden_units, activation=tf.nn.relu)\n",
    "hl = tf.nn.dropout(hl,keep_prob=(0.5))\n",
    "logits = tf.layers.dense(hl, 2)   # binary classification\n",
    "prob = tf.nn.softmax(logits)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calc loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = tf.placeholder(tf.int32, shape=(None))   # binary class labels\n",
    "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\n",
    "mean_loss = tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tips:** *sparse_softmax_cross_entropy_with_logits* requires index of labels while *softmax_cross_entropy_with_logits(_v2)* requires distributions of labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=hp.lr, beta1=0.9, beta2=0.98, epsilon=1e-8)\n",
    "train_op = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train neural network with very simple datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[array([[1, 4, 5, 8, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 2, 6, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), array([[ 1,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 9, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])], [array([[ 1,  4,  5,  5,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 1,  4, 10,  5,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]), array([[10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])], [array([[4, 4, 5, 6, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [9, 4, 2, 6, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), array([[ 1,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 5, 10,  4, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])], [array([[ 4,  4,  5,  8,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 1,  4, 10,  5,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]), array([[1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])]]\n",
      "[array([0, 1]), array([1, 1]), array([0, 1]), array([0, 0])]\n",
      "Tensor(\"Placeholder_2:0\", dtype=int32)\n",
      "*** Epoch 1 ***\n",
      "Epoch: 1, Mini-batch: 0, loss: 0.696927\n",
      "Epoch: 1, Mini-batch: 1, loss: 0.693104\n",
      "Epoch: 1, Mini-batch: 2, loss: 0.692431\n",
      "Epoch: 1, Mini-batch: 3, loss: 0.693219\n",
      "*** Epoch 2 ***\n",
      "Epoch: 2, Mini-batch: 0, loss: 0.689768\n",
      "Epoch: 2, Mini-batch: 1, loss: 0.692424\n",
      "Epoch: 2, Mini-batch: 2, loss: 0.691617\n",
      "Epoch: 2, Mini-batch: 3, loss: 0.693231\n",
      "*** Epoch 3 ***\n",
      "Epoch: 3, Mini-batch: 0, loss: 0.690610\n",
      "Epoch: 3, Mini-batch: 1, loss: 0.691743\n",
      "Epoch: 3, Mini-batch: 2, loss: 0.692576\n",
      "Epoch: 3, Mini-batch: 3, loss: 0.694722\n",
      "*** Epoch 4 ***\n",
      "Epoch: 4, Mini-batch: 0, loss: 0.690481\n",
      "Epoch: 4, Mini-batch: 1, loss: 0.692515\n",
      "Epoch: 4, Mini-batch: 2, loss: 0.688600\n",
      "Epoch: 4, Mini-batch: 3, loss: 0.693470\n",
      "*** Epoch 5 ***\n",
      "Epoch: 5, Mini-batch: 0, loss: 0.695560\n",
      "Epoch: 5, Mini-batch: 1, loss: 0.690518\n",
      "Epoch: 5, Mini-batch: 2, loss: 0.690378\n",
      "Epoch: 5, Mini-batch: 3, loss: 0.693477\n",
      "*** Epoch 6 ***\n",
      "Epoch: 6, Mini-batch: 0, loss: 0.692952\n",
      "Epoch: 6, Mini-batch: 1, loss: 0.693299\n",
      "Epoch: 6, Mini-batch: 2, loss: 0.691026\n",
      "Epoch: 6, Mini-batch: 3, loss: 0.692280\n",
      "*** Epoch 7 ***\n",
      "Epoch: 7, Mini-batch: 0, loss: 0.687973\n",
      "Epoch: 7, Mini-batch: 1, loss: 0.693085\n",
      "Epoch: 7, Mini-batch: 2, loss: 0.686964\n",
      "Epoch: 7, Mini-batch: 3, loss: 0.692303\n",
      "*** Epoch 8 ***\n",
      "Epoch: 8, Mini-batch: 0, loss: 0.690058\n",
      "Epoch: 8, Mini-batch: 1, loss: 0.690931\n",
      "Epoch: 8, Mini-batch: 2, loss: 0.692914\n",
      "Epoch: 8, Mini-batch: 3, loss: 0.691871\n",
      "*** Epoch 9 ***\n",
      "Epoch: 9, Mini-batch: 0, loss: 0.692719\n",
      "Epoch: 9, Mini-batch: 1, loss: 0.690207\n",
      "Epoch: 9, Mini-batch: 2, loss: 0.689976\n",
      "Epoch: 9, Mini-batch: 3, loss: 0.693651\n",
      "*** Epoch 10 ***\n",
      "Epoch: 10, Mini-batch: 0, loss: 0.689753\n",
      "Epoch: 10, Mini-batch: 1, loss: 0.687363\n",
      "Epoch: 10, Mini-batch: 2, loss: 0.686798\n",
      "Epoch: 10, Mini-batch: 3, loss: 0.696293\n",
      "*** Epoch 11 ***\n",
      "Epoch: 11, Mini-batch: 0, loss: 0.692858\n",
      "Epoch: 11, Mini-batch: 1, loss: 0.689954\n",
      "Epoch: 11, Mini-batch: 2, loss: 0.688697\n",
      "Epoch: 11, Mini-batch: 3, loss: 0.697170\n",
      "*** Epoch 12 ***\n",
      "Epoch: 12, Mini-batch: 0, loss: 0.689456\n",
      "Epoch: 12, Mini-batch: 1, loss: 0.685711\n",
      "Epoch: 12, Mini-batch: 2, loss: 0.694685\n",
      "Epoch: 12, Mini-batch: 3, loss: 0.697459\n",
      "*** Epoch 13 ***\n",
      "Epoch: 13, Mini-batch: 0, loss: 0.696678\n",
      "Epoch: 13, Mini-batch: 1, loss: 0.694010\n",
      "Epoch: 13, Mini-batch: 2, loss: 0.684825\n",
      "Epoch: 13, Mini-batch: 3, loss: 0.694952\n",
      "*** Epoch 14 ***\n",
      "Epoch: 14, Mini-batch: 0, loss: 0.696598\n",
      "Epoch: 14, Mini-batch: 1, loss: 0.689139\n",
      "Epoch: 14, Mini-batch: 2, loss: 0.689318\n",
      "Epoch: 14, Mini-batch: 3, loss: 0.691221\n",
      "*** Epoch 15 ***\n",
      "Epoch: 15, Mini-batch: 0, loss: 0.689052\n",
      "Epoch: 15, Mini-batch: 1, loss: 0.686910\n",
      "Epoch: 15, Mini-batch: 2, loss: 0.680924\n",
      "Epoch: 15, Mini-batch: 3, loss: 0.697211\n",
      "*** Epoch 16 ***\n",
      "Epoch: 16, Mini-batch: 0, loss: 0.685756\n",
      "Epoch: 16, Mini-batch: 1, loss: 0.693493\n",
      "Epoch: 16, Mini-batch: 2, loss: 0.682393\n",
      "Epoch: 16, Mini-batch: 3, loss: 0.693661\n",
      "*** Epoch 17 ***\n",
      "Epoch: 17, Mini-batch: 0, loss: 0.696144\n",
      "Epoch: 17, Mini-batch: 1, loss: 0.690119\n",
      "Epoch: 17, Mini-batch: 2, loss: 0.682656\n",
      "Epoch: 17, Mini-batch: 3, loss: 0.692794\n",
      "*** Epoch 18 ***\n",
      "Epoch: 18, Mini-batch: 0, loss: 0.685971\n",
      "Epoch: 18, Mini-batch: 1, loss: 0.690905\n",
      "Epoch: 18, Mini-batch: 2, loss: 0.686927\n",
      "Epoch: 18, Mini-batch: 3, loss: 0.691979\n",
      "*** Epoch 19 ***\n",
      "Epoch: 19, Mini-batch: 0, loss: 0.692528\n",
      "Epoch: 19, Mini-batch: 1, loss: 0.688641\n",
      "Epoch: 19, Mini-batch: 2, loss: 0.693060\n",
      "Epoch: 19, Mini-batch: 3, loss: 0.691174\n",
      "*** Epoch 20 ***\n",
      "Epoch: 20, Mini-batch: 0, loss: 0.696185\n",
      "Epoch: 20, Mini-batch: 1, loss: 0.689118\n",
      "Epoch: 20, Mini-batch: 2, loss: 0.684465\n",
      "Epoch: 20, Mini-batch: 3, loss: 0.695184\n",
      "*** Epoch 21 ***\n",
      "Epoch: 21, Mini-batch: 0, loss: 0.692412\n",
      "Epoch: 21, Mini-batch: 1, loss: 0.690049\n",
      "Epoch: 21, Mini-batch: 2, loss: 0.683832\n",
      "Epoch: 21, Mini-batch: 3, loss: 0.695331\n",
      "*** Epoch 22 ***\n",
      "Epoch: 22, Mini-batch: 0, loss: 0.684340\n",
      "Epoch: 22, Mini-batch: 1, loss: 0.694496\n",
      "Epoch: 22, Mini-batch: 2, loss: 0.685391\n",
      "Epoch: 22, Mini-batch: 3, loss: 0.697166\n",
      "*** Epoch 23 ***\n",
      "Epoch: 23, Mini-batch: 0, loss: 0.695678\n",
      "Epoch: 23, Mini-batch: 1, loss: 0.687557\n",
      "Epoch: 23, Mini-batch: 2, loss: 0.691952\n",
      "Epoch: 23, Mini-batch: 3, loss: 0.697356\n",
      "*** Epoch 24 ***\n",
      "Epoch: 24, Mini-batch: 0, loss: 0.693542\n",
      "Epoch: 24, Mini-batch: 1, loss: 0.687102\n",
      "Epoch: 24, Mini-batch: 2, loss: 0.696184\n",
      "Epoch: 24, Mini-batch: 3, loss: 0.691194\n",
      "*** Epoch 25 ***\n",
      "Epoch: 25, Mini-batch: 0, loss: 0.695956\n",
      "Epoch: 25, Mini-batch: 1, loss: 0.693839\n",
      "Epoch: 25, Mini-batch: 2, loss: 0.686393\n",
      "Epoch: 25, Mini-batch: 3, loss: 0.690292\n",
      "*** Epoch 26 ***\n",
      "Epoch: 26, Mini-batch: 0, loss: 0.687802\n",
      "Epoch: 26, Mini-batch: 1, loss: 0.689427\n",
      "Epoch: 26, Mini-batch: 2, loss: 0.688052\n",
      "Epoch: 26, Mini-batch: 3, loss: 0.693712\n",
      "*** Epoch 27 ***\n",
      "Epoch: 27, Mini-batch: 0, loss: 0.696783\n",
      "Epoch: 27, Mini-batch: 1, loss: 0.689535\n",
      "Epoch: 27, Mini-batch: 2, loss: 0.685705\n",
      "Epoch: 27, Mini-batch: 3, loss: 0.694450\n",
      "*** Epoch 28 ***\n",
      "Epoch: 28, Mini-batch: 0, loss: 0.688403\n",
      "Epoch: 28, Mini-batch: 1, loss: 0.694821\n",
      "Epoch: 28, Mini-batch: 2, loss: 0.680964\n",
      "Epoch: 28, Mini-batch: 3, loss: 0.691326\n",
      "*** Epoch 29 ***\n",
      "Epoch: 29, Mini-batch: 0, loss: 0.692664\n",
      "Epoch: 29, Mini-batch: 1, loss: 0.693492\n",
      "Epoch: 29, Mini-batch: 2, loss: 0.681511\n",
      "Epoch: 29, Mini-batch: 3, loss: 0.691267\n",
      "*** Epoch 30 ***\n",
      "Epoch: 30, Mini-batch: 0, loss: 0.695992\n",
      "Epoch: 30, Mini-batch: 1, loss: 0.689940\n",
      "Epoch: 30, Mini-batch: 2, loss: 0.692821\n",
      "Epoch: 30, Mini-batch: 3, loss: 0.697943\n",
      "*** Epoch 31 ***\n",
      "Epoch: 31, Mini-batch: 0, loss: 0.688067\n",
      "Epoch: 31, Mini-batch: 1, loss: 0.689239\n",
      "Epoch: 31, Mini-batch: 2, loss: 0.688143\n",
      "Epoch: 31, Mini-batch: 3, loss: 0.698275\n",
      "*** Epoch 32 ***\n",
      "Epoch: 32, Mini-batch: 0, loss: 0.696829\n",
      "Epoch: 32, Mini-batch: 1, loss: 0.693292\n",
      "Epoch: 32, Mini-batch: 2, loss: 0.690199\n",
      "Epoch: 32, Mini-batch: 3, loss: 0.694423\n",
      "*** Epoch 33 ***\n",
      "Epoch: 33, Mini-batch: 0, loss: 0.692677\n",
      "Epoch: 33, Mini-batch: 1, loss: 0.686339\n",
      "Epoch: 33, Mini-batch: 2, loss: 0.691719\n",
      "Epoch: 33, Mini-batch: 3, loss: 0.690017\n",
      "*** Epoch 34 ***\n",
      "Epoch: 34, Mini-batch: 0, loss: 0.687837\n",
      "Epoch: 34, Mini-batch: 1, loss: 0.690541\n",
      "Epoch: 34, Mini-batch: 2, loss: 0.690105\n",
      "Epoch: 34, Mini-batch: 3, loss: 0.689838\n",
      "*** Epoch 35 ***\n",
      "Epoch: 35, Mini-batch: 0, loss: 0.684363\n",
      "Epoch: 35, Mini-batch: 1, loss: 0.693863\n",
      "Epoch: 35, Mini-batch: 2, loss: 0.676418\n",
      "Epoch: 35, Mini-batch: 3, loss: 0.694295\n",
      "*** Epoch 36 ***\n",
      "Epoch: 36, Mini-batch: 0, loss: 0.696994\n",
      "Epoch: 36, Mini-batch: 1, loss: 0.684558\n",
      "Epoch: 36, Mini-batch: 2, loss: 0.692787\n",
      "Epoch: 36, Mini-batch: 3, loss: 0.692950\n",
      "*** Epoch 37 ***\n",
      "Epoch: 37, Mini-batch: 0, loss: 0.683313\n",
      "Epoch: 37, Mini-batch: 1, loss: 0.683923\n",
      "Epoch: 37, Mini-batch: 2, loss: 0.677903\n",
      "Epoch: 37, Mini-batch: 3, loss: 0.691167\n",
      "*** Epoch 38 ***\n",
      "Epoch: 38, Mini-batch: 0, loss: 0.683048\n",
      "Epoch: 38, Mini-batch: 1, loss: 0.690088\n",
      "Epoch: 38, Mini-batch: 2, loss: 0.693631\n",
      "Epoch: 38, Mini-batch: 3, loss: 0.696080\n",
      "*** Epoch 39 ***\n",
      "Epoch: 39, Mini-batch: 0, loss: 0.697088\n",
      "Epoch: 39, Mini-batch: 1, loss: 0.687684\n",
      "Epoch: 39, Mini-batch: 2, loss: 0.673047\n",
      "Epoch: 39, Mini-batch: 3, loss: 0.693921\n",
      "*** Epoch 40 ***\n",
      "Epoch: 40, Mini-batch: 0, loss: 0.687578\n",
      "Epoch: 40, Mini-batch: 1, loss: 0.681681\n",
      "Epoch: 40, Mini-batch: 2, loss: 0.679139\n",
      "Epoch: 40, Mini-batch: 3, loss: 0.696332\n",
      "*** Epoch 41 ***\n",
      "Epoch: 41, Mini-batch: 0, loss: 0.680895\n",
      "Epoch: 41, Mini-batch: 1, loss: 0.692395\n",
      "Epoch: 41, Mini-batch: 2, loss: 0.695111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41, Mini-batch: 3, loss: 0.690608\n",
      "*** Epoch 42 ***\n",
      "Epoch: 42, Mini-batch: 0, loss: 0.692244\n",
      "Epoch: 42, Mini-batch: 1, loss: 0.687476\n",
      "Epoch: 42, Mini-batch: 2, loss: 0.676173\n",
      "Epoch: 42, Mini-batch: 3, loss: 0.689260\n",
      "*** Epoch 43 ***\n",
      "Epoch: 43, Mini-batch: 0, loss: 0.697647\n",
      "Epoch: 43, Mini-batch: 1, loss: 0.689338\n",
      "Epoch: 43, Mini-batch: 2, loss: 0.686948\n",
      "Epoch: 43, Mini-batch: 3, loss: 0.698713\n",
      "*** Epoch 44 ***\n",
      "Epoch: 44, Mini-batch: 0, loss: 0.682722\n",
      "Epoch: 44, Mini-batch: 1, loss: 0.694362\n",
      "Epoch: 44, Mini-batch: 2, loss: 0.683286\n",
      "Epoch: 44, Mini-batch: 3, loss: 0.688796\n",
      "*** Epoch 45 ***\n",
      "Epoch: 45, Mini-batch: 0, loss: 0.681543\n",
      "Epoch: 45, Mini-batch: 1, loss: 0.687866\n",
      "Epoch: 45, Mini-batch: 2, loss: 0.685993\n",
      "Epoch: 45, Mini-batch: 3, loss: 0.696398\n",
      "*** Epoch 46 ***\n",
      "Epoch: 46, Mini-batch: 0, loss: 0.686898\n",
      "Epoch: 46, Mini-batch: 1, loss: 0.688803\n",
      "Epoch: 46, Mini-batch: 2, loss: 0.684583\n",
      "Epoch: 46, Mini-batch: 3, loss: 0.698063\n",
      "*** Epoch 47 ***\n",
      "Epoch: 47, Mini-batch: 0, loss: 0.692024\n",
      "Epoch: 47, Mini-batch: 1, loss: 0.694896\n",
      "Epoch: 47, Mini-batch: 2, loss: 0.691180\n",
      "Epoch: 47, Mini-batch: 3, loss: 0.699677\n",
      "*** Epoch 48 ***\n",
      "Epoch: 48, Mini-batch: 0, loss: 0.696468\n",
      "Epoch: 48, Mini-batch: 1, loss: 0.682347\n",
      "Epoch: 48, Mini-batch: 2, loss: 0.685655\n",
      "Epoch: 48, Mini-batch: 3, loss: 0.688123\n",
      "*** Epoch 49 ***\n",
      "Epoch: 49, Mini-batch: 0, loss: 0.693725\n",
      "Epoch: 49, Mini-batch: 1, loss: 0.689793\n",
      "Epoch: 49, Mini-batch: 2, loss: 0.694874\n",
      "Epoch: 49, Mini-batch: 3, loss: 0.696506\n",
      "*** Epoch 50 ***\n",
      "Epoch: 50, Mini-batch: 0, loss: 0.690659\n",
      "Epoch: 50, Mini-batch: 1, loss: 0.689625\n",
      "Epoch: 50, Mini-batch: 2, loss: 0.689647\n",
      "Epoch: 50, Mini-batch: 3, loss: 0.699529\n",
      "*** Epoch 51 ***\n",
      "Epoch: 51, Mini-batch: 0, loss: 0.697409\n",
      "Epoch: 51, Mini-batch: 1, loss: 0.688324\n",
      "Epoch: 51, Mini-batch: 2, loss: 0.679548\n",
      "Epoch: 51, Mini-batch: 3, loss: 0.695098\n",
      "*** Epoch 52 ***\n",
      "Epoch: 52, Mini-batch: 0, loss: 0.697166\n",
      "Epoch: 52, Mini-batch: 1, loss: 0.682082\n",
      "Epoch: 52, Mini-batch: 2, loss: 0.689375\n",
      "Epoch: 52, Mini-batch: 3, loss: 0.700009\n",
      "*** Epoch 53 ***\n",
      "Epoch: 53, Mini-batch: 0, loss: 0.692438\n",
      "Epoch: 53, Mini-batch: 1, loss: 0.682658\n",
      "Epoch: 53, Mini-batch: 2, loss: 0.691366\n",
      "Epoch: 53, Mini-batch: 3, loss: 0.694768\n",
      "*** Epoch 54 ***\n",
      "Epoch: 54, Mini-batch: 0, loss: 0.695759\n",
      "Epoch: 54, Mini-batch: 1, loss: 0.687145\n",
      "Epoch: 54, Mini-batch: 2, loss: 0.676677\n",
      "Epoch: 54, Mini-batch: 3, loss: 0.700018\n",
      "*** Epoch 55 ***\n",
      "Epoch: 55, Mini-batch: 0, loss: 0.679973\n",
      "Epoch: 55, Mini-batch: 1, loss: 0.688656\n",
      "Epoch: 55, Mini-batch: 2, loss: 0.669174\n",
      "Epoch: 55, Mini-batch: 3, loss: 0.693488\n",
      "*** Epoch 56 ***\n",
      "Epoch: 56, Mini-batch: 0, loss: 0.678758\n",
      "Epoch: 56, Mini-batch: 1, loss: 0.689536\n",
      "Epoch: 56, Mini-batch: 2, loss: 0.669701\n",
      "Epoch: 56, Mini-batch: 3, loss: 0.699638\n",
      "*** Epoch 57 ***\n",
      "Epoch: 57, Mini-batch: 0, loss: 0.685707\n",
      "Epoch: 57, Mini-batch: 1, loss: 0.681415\n",
      "Epoch: 57, Mini-batch: 2, loss: 0.672356\n",
      "Epoch: 57, Mini-batch: 3, loss: 0.691425\n",
      "*** Epoch 58 ***\n",
      "Epoch: 58, Mini-batch: 0, loss: 0.697866\n",
      "Epoch: 58, Mini-batch: 1, loss: 0.686775\n",
      "Epoch: 58, Mini-batch: 2, loss: 0.695913\n",
      "Epoch: 58, Mini-batch: 3, loss: 0.694279\n",
      "*** Epoch 59 ***\n",
      "Epoch: 59, Mini-batch: 0, loss: 0.685903\n",
      "Epoch: 59, Mini-batch: 1, loss: 0.679062\n",
      "Epoch: 59, Mini-batch: 2, loss: 0.694098\n",
      "Epoch: 59, Mini-batch: 3, loss: 0.696361\n",
      "*** Epoch 60 ***\n",
      "Epoch: 60, Mini-batch: 0, loss: 0.696062\n",
      "Epoch: 60, Mini-batch: 1, loss: 0.688565\n",
      "Epoch: 60, Mini-batch: 2, loss: 0.689438\n",
      "Epoch: 60, Mini-batch: 3, loss: 0.694137\n",
      "*** Epoch 61 ***\n",
      "Epoch: 61, Mini-batch: 0, loss: 0.691032\n",
      "Epoch: 61, Mini-batch: 1, loss: 0.687784\n",
      "Epoch: 61, Mini-batch: 2, loss: 0.667516\n",
      "Epoch: 61, Mini-batch: 3, loss: 0.698404\n",
      "*** Epoch 62 ***\n",
      "Epoch: 62, Mini-batch: 0, loss: 0.679591\n",
      "Epoch: 62, Mini-batch: 1, loss: 0.682144\n",
      "Epoch: 62, Mini-batch: 2, loss: 0.688672\n",
      "Epoch: 62, Mini-batch: 3, loss: 0.698377\n",
      "*** Epoch 63 ***\n",
      "Epoch: 63, Mini-batch: 0, loss: 0.697518\n",
      "Epoch: 63, Mini-batch: 1, loss: 0.687172\n",
      "Epoch: 63, Mini-batch: 2, loss: 0.669946\n",
      "Epoch: 63, Mini-batch: 3, loss: 0.694156\n",
      "*** Epoch 64 ***\n",
      "Epoch: 64, Mini-batch: 0, loss: 0.686429\n",
      "Epoch: 64, Mini-batch: 1, loss: 0.687195\n",
      "Epoch: 64, Mini-batch: 2, loss: 0.679599\n",
      "Epoch: 64, Mini-batch: 3, loss: 0.698423\n",
      "*** Epoch 65 ***\n",
      "Epoch: 65, Mini-batch: 0, loss: 0.697486\n",
      "Epoch: 65, Mini-batch: 1, loss: 0.696096\n",
      "Epoch: 65, Mini-batch: 2, loss: 0.670378\n",
      "Epoch: 65, Mini-batch: 3, loss: 0.692875\n",
      "*** Epoch 66 ***\n",
      "Epoch: 66, Mini-batch: 0, loss: 0.697905\n",
      "Epoch: 66, Mini-batch: 1, loss: 0.687365\n",
      "Epoch: 66, Mini-batch: 2, loss: 0.678602\n",
      "Epoch: 66, Mini-batch: 3, loss: 0.688698\n",
      "*** Epoch 67 ***\n",
      "Epoch: 67, Mini-batch: 0, loss: 0.679717\n",
      "Epoch: 67, Mini-batch: 1, loss: 0.697792\n",
      "Epoch: 67, Mini-batch: 2, loss: 0.681100\n",
      "Epoch: 67, Mini-batch: 3, loss: 0.698051\n",
      "*** Epoch 68 ***\n",
      "Epoch: 68, Mini-batch: 0, loss: 0.693690\n",
      "Epoch: 68, Mini-batch: 1, loss: 0.689633\n",
      "Epoch: 68, Mini-batch: 2, loss: 0.696490\n",
      "Epoch: 68, Mini-batch: 3, loss: 0.694201\n",
      "*** Epoch 69 ***\n",
      "Epoch: 69, Mini-batch: 0, loss: 0.677778\n",
      "Epoch: 69, Mini-batch: 1, loss: 0.688993\n",
      "Epoch: 69, Mini-batch: 2, loss: 0.670161\n",
      "Epoch: 69, Mini-batch: 3, loss: 0.688858\n",
      "*** Epoch 70 ***\n",
      "Epoch: 70, Mini-batch: 0, loss: 0.675847\n",
      "Epoch: 70, Mini-batch: 1, loss: 0.696346\n",
      "Epoch: 70, Mini-batch: 2, loss: 0.666895\n",
      "Epoch: 70, Mini-batch: 3, loss: 0.694247\n",
      "*** Epoch 71 ***\n",
      "Epoch: 71, Mini-batch: 0, loss: 0.685869\n",
      "Epoch: 71, Mini-batch: 1, loss: 0.687918\n",
      "Epoch: 71, Mini-batch: 2, loss: 0.689759\n",
      "Epoch: 71, Mini-batch: 3, loss: 0.691299\n",
      "*** Epoch 72 ***\n",
      "Epoch: 72, Mini-batch: 0, loss: 0.697766\n",
      "Epoch: 72, Mini-batch: 1, loss: 0.694989\n",
      "Epoch: 72, Mini-batch: 2, loss: 0.666231\n",
      "Epoch: 72, Mini-batch: 3, loss: 0.686966\n",
      "*** Epoch 73 ***\n",
      "Epoch: 73, Mini-batch: 0, loss: 0.678815\n",
      "Epoch: 73, Mini-batch: 1, loss: 0.686606\n",
      "Epoch: 73, Mini-batch: 2, loss: 0.673476\n",
      "Epoch: 73, Mini-batch: 3, loss: 0.694150\n",
      "*** Epoch 74 ***\n",
      "Epoch: 74, Mini-batch: 0, loss: 0.684500\n",
      "Epoch: 74, Mini-batch: 1, loss: 0.676690\n",
      "Epoch: 74, Mini-batch: 2, loss: 0.679126\n",
      "Epoch: 74, Mini-batch: 3, loss: 0.694665\n",
      "*** Epoch 75 ***\n",
      "Epoch: 75, Mini-batch: 0, loss: 0.684605\n",
      "Epoch: 75, Mini-batch: 1, loss: 0.687845\n",
      "Epoch: 75, Mini-batch: 2, loss: 0.680849\n",
      "Epoch: 75, Mini-batch: 3, loss: 0.685688\n",
      "*** Epoch 76 ***\n",
      "Epoch: 76, Mini-batch: 0, loss: 0.678786\n",
      "Epoch: 76, Mini-batch: 1, loss: 0.684839\n",
      "Epoch: 76, Mini-batch: 2, loss: 0.693093\n",
      "Epoch: 76, Mini-batch: 3, loss: 0.699606\n",
      "*** Epoch 77 ***\n",
      "Epoch: 77, Mini-batch: 0, loss: 0.697261\n",
      "Epoch: 77, Mini-batch: 1, loss: 0.691974\n",
      "Epoch: 77, Mini-batch: 2, loss: 0.661876\n",
      "Epoch: 77, Mini-batch: 3, loss: 0.693255\n",
      "*** Epoch 78 ***\n",
      "Epoch: 78, Mini-batch: 0, loss: 0.681281\n",
      "Epoch: 78, Mini-batch: 1, loss: 0.680609\n",
      "Epoch: 78, Mini-batch: 2, loss: 0.696906\n",
      "Epoch: 78, Mini-batch: 3, loss: 0.693155\n",
      "*** Epoch 79 ***\n",
      "Epoch: 79, Mini-batch: 0, loss: 0.679824\n",
      "Epoch: 79, Mini-batch: 1, loss: 0.687589\n",
      "Epoch: 79, Mini-batch: 2, loss: 0.674556\n",
      "Epoch: 79, Mini-batch: 3, loss: 0.695691\n",
      "*** Epoch 80 ***\n",
      "Epoch: 80, Mini-batch: 0, loss: 0.693191\n",
      "Epoch: 80, Mini-batch: 1, loss: 0.682068\n",
      "Epoch: 80, Mini-batch: 2, loss: 0.675485\n",
      "Epoch: 80, Mini-batch: 3, loss: 0.695988\n",
      "*** Epoch 81 ***\n",
      "Epoch: 81, Mini-batch: 0, loss: 0.669775\n",
      "Epoch: 81, Mini-batch: 1, loss: 0.686945\n",
      "Epoch: 81, Mini-batch: 2, loss: 0.659399\n",
      "Epoch: 81, Mini-batch: 3, loss: 0.705007\n",
      "*** Epoch 82 ***\n",
      "Epoch: 82, Mini-batch: 0, loss: 0.676620\n",
      "Epoch: 82, Mini-batch: 1, loss: 0.684797\n",
      "Epoch: 82, Mini-batch: 2, loss: 0.684332\n",
      "Epoch: 82, Mini-batch: 3, loss: 0.691176\n",
      "*** Epoch 83 ***\n",
      "Epoch: 83, Mini-batch: 0, loss: 0.675678\n",
      "Epoch: 83, Mini-batch: 1, loss: 0.677281\n",
      "Epoch: 83, Mini-batch: 2, loss: 0.684157\n",
      "Epoch: 83, Mini-batch: 3, loss: 0.700450\n",
      "*** Epoch 84 ***\n",
      "Epoch: 84, Mini-batch: 0, loss: 0.670266\n",
      "Epoch: 84, Mini-batch: 1, loss: 0.676416\n",
      "Epoch: 84, Mini-batch: 2, loss: 0.691638\n",
      "Epoch: 84, Mini-batch: 3, loss: 0.699599\n",
      "*** Epoch 85 ***\n",
      "Epoch: 85, Mini-batch: 0, loss: 0.683019\n",
      "Epoch: 85, Mini-batch: 1, loss: 0.679252\n",
      "Epoch: 85, Mini-batch: 2, loss: 0.685331\n",
      "Epoch: 85, Mini-batch: 3, loss: 0.694262\n",
      "*** Epoch 86 ***\n",
      "Epoch: 86, Mini-batch: 0, loss: 0.693864\n",
      "Epoch: 86, Mini-batch: 1, loss: 0.681626\n",
      "Epoch: 86, Mini-batch: 2, loss: 0.688548\n",
      "Epoch: 86, Mini-batch: 3, loss: 0.686369\n",
      "*** Epoch 87 ***\n",
      "Epoch: 87, Mini-batch: 0, loss: 0.700688\n",
      "Epoch: 87, Mini-batch: 1, loss: 0.682018\n",
      "Epoch: 87, Mini-batch: 2, loss: 0.664884\n",
      "Epoch: 87, Mini-batch: 3, loss: 0.688372\n",
      "*** Epoch 88 ***\n",
      "Epoch: 88, Mini-batch: 0, loss: 0.700709\n",
      "Epoch: 88, Mini-batch: 1, loss: 0.672154\n",
      "Epoch: 88, Mini-batch: 2, loss: 0.679018\n",
      "Epoch: 88, Mini-batch: 3, loss: 0.691092\n",
      "*** Epoch 89 ***\n",
      "Epoch: 89, Mini-batch: 0, loss: 0.702122\n",
      "Epoch: 89, Mini-batch: 1, loss: 0.669470\n",
      "Epoch: 89, Mini-batch: 2, loss: 0.662304\n",
      "Epoch: 89, Mini-batch: 3, loss: 0.703366\n",
      "*** Epoch 90 ***\n",
      "Epoch: 90, Mini-batch: 0, loss: 0.680609\n",
      "Epoch: 90, Mini-batch: 1, loss: 0.683814\n",
      "Epoch: 90, Mini-batch: 2, loss: 0.656370\n",
      "Epoch: 90, Mini-batch: 3, loss: 0.705740\n",
      "*** Epoch 91 ***\n",
      "Epoch: 91, Mini-batch: 0, loss: 0.661568\n",
      "Epoch: 91, Mini-batch: 1, loss: 0.677348\n",
      "Epoch: 91, Mini-batch: 2, loss: 0.683143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 91, Mini-batch: 3, loss: 0.699487\n",
      "*** Epoch 92 ***\n",
      "Epoch: 92, Mini-batch: 0, loss: 0.696555\n",
      "Epoch: 92, Mini-batch: 1, loss: 0.664860\n",
      "Epoch: 92, Mini-batch: 2, loss: 0.655470\n",
      "Epoch: 92, Mini-batch: 3, loss: 0.708998\n",
      "*** Epoch 93 ***\n",
      "Epoch: 93, Mini-batch: 0, loss: 0.694447\n",
      "Epoch: 93, Mini-batch: 1, loss: 0.674852\n",
      "Epoch: 93, Mini-batch: 2, loss: 0.673648\n",
      "Epoch: 93, Mini-batch: 3, loss: 0.686788\n",
      "*** Epoch 94 ***\n",
      "Epoch: 94, Mini-batch: 0, loss: 0.692743\n",
      "Epoch: 94, Mini-batch: 1, loss: 0.694160\n",
      "Epoch: 94, Mini-batch: 2, loss: 0.662378\n",
      "Epoch: 94, Mini-batch: 3, loss: 0.706325\n",
      "*** Epoch 95 ***\n",
      "Epoch: 95, Mini-batch: 0, loss: 0.685980\n",
      "Epoch: 95, Mini-batch: 1, loss: 0.672836\n",
      "Epoch: 95, Mini-batch: 2, loss: 0.686874\n",
      "Epoch: 95, Mini-batch: 3, loss: 0.713110\n",
      "*** Epoch 96 ***\n",
      "Epoch: 96, Mini-batch: 0, loss: 0.669144\n",
      "Epoch: 96, Mini-batch: 1, loss: 0.684293\n",
      "Epoch: 96, Mini-batch: 2, loss: 0.658630\n",
      "Epoch: 96, Mini-batch: 3, loss: 0.699286\n",
      "*** Epoch 97 ***\n",
      "Epoch: 97, Mini-batch: 0, loss: 0.683359\n",
      "Epoch: 97, Mini-batch: 1, loss: 0.672796\n",
      "Epoch: 97, Mini-batch: 2, loss: 0.671198\n",
      "Epoch: 97, Mini-batch: 3, loss: 0.690762\n",
      "*** Epoch 98 ***\n",
      "Epoch: 98, Mini-batch: 0, loss: 0.682052\n",
      "Epoch: 98, Mini-batch: 1, loss: 0.687386\n",
      "Epoch: 98, Mini-batch: 2, loss: 0.696289\n",
      "Epoch: 98, Mini-batch: 3, loss: 0.704934\n",
      "*** Epoch 99 ***\n",
      "Epoch: 99, Mini-batch: 0, loss: 0.679404\n",
      "Epoch: 99, Mini-batch: 1, loss: 0.672016\n",
      "Epoch: 99, Mini-batch: 2, loss: 0.669692\n",
      "Epoch: 99, Mini-batch: 3, loss: 0.699806\n",
      "*** Epoch 100 ***\n",
      "Epoch: 100, Mini-batch: 0, loss: 0.668593\n",
      "Epoch: 100, Mini-batch: 1, loss: 0.691061\n",
      "Epoch: 100, Mini-batch: 2, loss: 0.665702\n",
      "Epoch: 100, Mini-batch: 3, loss: 0.693079\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "datasets=[\n",
    "        [np.array([ [1,4,5,8,3]+[0 for _ in range(11)], [1,1,2,6,3]+[0 for _ in range(11)] ]),\n",
    "         np.array([ [1,2]+[0 for _ in range(14)],       [9,10]+[0 for _ in range(14)]])],\n",
    "        [np.array([ [1,4,5,5,3]+[0 for _ in range(11)], [1,4,10,5,3]+[0 for _ in range(11)] ]),\n",
    "         np.array([ [10]+[0 for _ in range(15)],        [10]+[0 for _ in range(15)]])],\n",
    "        [np.array([ [4,4,5,6,3]+[0 for _ in range(11)], [9,4,2,6,3]+[0 for _ in range(11)] ]),\n",
    "         np.array([ [1,2]+[0 for _ in range(14)],       [5,10,4,10]+[0 for _ in range(12)]])],\n",
    "        [np.array([ [4,4,5,8,3]+[0 for _ in range(11)], [1,4,10,5,3]+[0 for _ in range(11)] ]),\n",
    "         np.array([ [1,2]+[0 for _ in range(14)],        [1,2]+[0 for _ in range(14)]])],\n",
    "]\n",
    "labels=[np.array([0,1]), np.array([1,1]), np.array([0,1]), np.array([0,0])]\n",
    "print(datasets)\n",
    "print(labels)\n",
    "print(label)\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    print(\"*** Epoch %d ***\"%epoch)\n",
    "    for mb,(data,lab) in enumerate(zip(datasets,labels)):\n",
    "        feed_dict={\n",
    "                idx_q:data[0],\n",
    "                idx_a:data[1],\n",
    "                label:lab,\n",
    "                }\n",
    "        log_loss,log_train_op = sess.run([mean_loss,train_op],feed_dict=feed_dict)\n",
    "        print(\"Epoch: %d, Mini-batch: %d, loss: %f\"%(epoch,mb,log_loss))\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tips:** *sess.run(fetches,feed_dict)* is a function to execute real computations and evaluate (get) tensors specified as *fetches*, given the tensors by *feed_dict*. This can be used for training, testing and other computation operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with your net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability: [array([[0.48932728, 0.51067275],\n",
      "       [0.4773536 , 0.5226464 ]], dtype=float32)]\n",
      "Argmax: [[1 1]]\n",
      "Probability: [array([[0.50016475, 0.49983525],\n",
      "       [0.49704963, 0.5029503 ]], dtype=float32)]\n",
      "Argmax: [[0 1]]\n",
      "Probability: [array([[0.50519466, 0.49480534],\n",
      "       [0.4621528 , 0.5378471 ]], dtype=float32)]\n",
      "Argmax: [[0 1]]\n",
      "Probability: [array([[0.5000392 , 0.49996075],\n",
      "       [0.493157  , 0.50684303]], dtype=float32)]\n",
      "Argmax: [[0 1]]\n"
     ]
    }
   ],
   "source": [
    "for data,lab in zip(datasets,labels):\n",
    "    feed_dict={\n",
    "            idx_q:data[0],\n",
    "            idx_a:data[1],\n",
    "            label:lab,\n",
    "            }\n",
    "    log_prob = sess.run([prob],feed_dict=feed_dict)\n",
    "    log_class = np.argmax(log_prob,axis=-1)\n",
    "    print(\"Probability: \"+str(log_prob))\n",
    "    print(\"Argmax: \"+str(log_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, this dataset is too tiny to train this network..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign default values to the lookup table.\n",
    "Use sess.run() after creating a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lookup table.\n",
    "embedding_shape=(vocab_size, num_units)\n",
    "emb_w = tf.Variable(tf.constant(0.0, shape=embedding_shape), name=\"emb_w\")\n",
    "embedding_placeholder_w = tf.placeholder(tf.float32, embedding_shape)\n",
    "embedding_init_w = emb_w.assign(embedding_placeholder_w)      # `assign` operation.\n",
    "# ...\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# Run assign computation graph.\n",
    "init_wmat=np.random.randn(*embedding_shape)                   # Of course, you can swap this with gensim vec. \n",
    "sess.run([embedding_init_w], feed_dict={embedding_placeholder_w: init_wmat})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a special line for tensorboard. You don't need to run below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.FileWriter(logdir='./graph/', graph=tf.Graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir='./graph/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access localhost:PORT_NUM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips\n",
    "This is the BasicLSTM of Tensorflow. You might see in many websites introducing tensorflow.<br>\n",
    "However, experts rarely use this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input shape must be [mini_batch, time_sequence, embedding]\n",
    "def simple_stack_bilstm(input, num_stacked, dim_hidden, timesteps, dropout, name):\n",
    "   # Unstack to expand input to a list of 'timesteps' tensors of shape (mini_batch, embedding)\n",
    "    input = tf.unstack(input, timesteps, 1)\n",
    "    # Define lstm cells with tensorflow\n",
    "    lstm_fw_cell = [tf.contrib.rnn.BasicLSTMCell(dim_hidden, forget_bias=1.0, reuse=tf.AUTO_REUSE, name=name+\"_f_\"+str(i)) for i in range(num_stacked)]\n",
    "    lstm_bw_cell = [tf.contrib.rnn.BasicLSTMCell(dim_hidden, forget_bias=1.0, reuse=tf.AUTO_REUSE, name=name+\"_b_\"+str(i)) for i in range(num_stacked)]\n",
    "    # Define computation graph of LSTMs\n",
    "    outputs, _, _ = tf.contrib.rnn.stack_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, input, dtype=tf.float32)\n",
    "    outputs=tf.stack(outputs,axis=1)\n",
    "    return outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
